[["index.html", "RTI Take Home Assignment Chapter 1 SQL Data Consolidation", " RTI Take Home Assignment Louise Lindegaard 2022-02-06 Chapter 1 SQL Data Consolidation Main objective: Select all variables from the records table and join them with all other tables to consolidate 9 tables into 1 â€“ The select statement contains 2 data cleaning steps: 1. Remove redundant columns 2. Rename variables to logical names "],["model-building.html", "Chapter 2 Model Building", " Chapter 2 Model Building "],["import-data-and-libraries-into-r.html", "Chapter 3 Import data and libraries into R", " Chapter 3 Import data and libraries into R "],["data-cleaning.html", "Chapter 4 Data Cleaning", " Chapter 4 Data Cleaning "],["variable-binning.html", "Chapter 5 Variable binning", " Chapter 5 Variable binning "],["check-for-quasi-complete-separation.html", "Chapter 6 Check for Quasi Complete Separation", " Chapter 6 Check for Quasi Complete Separation "],["split-data-into-training-validation-and-test.html", "Chapter 7 Split data into training, validation and test", " Chapter 7 Split data into training, validation and test "],["exploratory-statistics-on-training-data.html", "Chapter 8 Exploratory Statistics on Training Data 8.1 Develop Logistic Regression Model", " Chapter 8 Exploratory Statistics on Training Data 8.1 Develop Logistic Regression Model "],["check-for-issues-with-multi-collinearity.html", "Chapter 9 Check for issues with multi-collinearity", " Chapter 9 Check for issues with multi-collinearity "],["variable-selection.html", "Chapter 10 Variable Selection 10.1 Evaluate logistic regression", " Chapter 10 Variable Selection # Stepwise selection to select relevant variables full.model &lt;- glm(over_50k ~ ., data=train[,2:13], family = binomial(link = &quot;logit&quot;)) empty.model &lt;- glm(over_50k ~ 1, data=train[,2:13], family = binomial(link = &quot;logit&quot;)) step.model &lt;- step(empty.model, scope = list(lower=formula(empty.model), upper=formula(full.model)), direction = &quot;both&quot;) ## Start: AIC=37694.44 ## over_50k ~ 1 ## ## Df Deviance AIC ## + marital_status 6 30300 30314 ## + education_level 14 33358 33388 ## + occupation 12 33453 33479 ## + age_bin 3 34099 34107 ## + hours_week_bin 2 35232 35238 ## + capital_gain_indicator 1 35585 35589 ## + sex 1 35968 35972 ## + workclass 6 36618 36632 ## + capital_loss_indicator 1 37155 37159 ## + race 4 37300 37310 ## + country_bin 1 37665 37669 ## &lt;none&gt; 37692 37694 ## ## Step: AIC=30314.02 ## over_50k ~ marital_status ## ## Df Deviance AIC ## + education_level 14 26282 26324 ## + occupation 12 26922 26960 ## + capital_gain_indicator 1 28752 28768 ## + age_bin 3 29140 29160 ## + hours_week_bin 2 29176 29194 ## + workclass 6 29724 29750 ## + capital_loss_indicator 1 29916 29932 ## + race 4 30168 30190 ## + sex 1 30236 30252 ## + country_bin 1 30248 30264 ## &lt;none&gt; 30300 30314 ## - marital_status 6 37692 37694 ## ## Step: AIC=26323.79 ## over_50k ~ marital_status + education_level ## ## Df Deviance AIC ## + capital_gain_indicator 1 25132 25176 ## + occupation 12 25302 25368 ## + age_bin 3 25479 25527 ## + hours_week_bin 2 25580 25626 ## + workclass 6 25958 26012 ## + capital_loss_indicator 1 26050 26094 ## + sex 1 26207 26251 ## + race 4 26231 26281 ## + country_bin 1 26244 26288 ## &lt;none&gt; 26282 26324 ## - education_level 14 30300 30314 ## - marital_status 6 33358 33388 ## ## Step: AIC=25175.95 ## over_50k ~ marital_status + education_level + capital_gain_indicator ## ## Df Deviance AIC ## + occupation 12 24204 24272 ## + age_bin 3 24393 24443 ## + hours_week_bin 2 24468 24516 ## + capital_loss_indicator 1 24794 24840 ## + workclass 6 24829 24885 ## + sex 1 25062 25108 ## + race 4 25085 25137 ## + country_bin 1 25099 25145 ## &lt;none&gt; 25132 25176 ## - capital_gain_indicator 1 26282 26324 ## - education_level 14 28752 28768 ## - marital_status 6 31808 31840 ## ## Step: AIC=24271.93 ## over_50k ~ marital_status + education_level + capital_gain_indicator + ## occupation ## ## Df Deviance AIC ## + age_bin 3 23557 23631 ## + hours_week_bin 2 23728 23800 ## + capital_loss_indicator 1 23894 23964 ## + workclass 6 24069 24149 ## + sex 1 24146 24216 ## + race 4 24176 24252 ## + country_bin 1 24186 24256 ## &lt;none&gt; 24204 24272 ## - occupation 12 25132 25176 ## - capital_gain_indicator 1 25302 25368 ## - education_level 14 25664 25704 ## - marital_status 6 30174 30230 ## ## Step: AIC=23630.63 ## over_50k ~ marital_status + education_level + capital_gain_indicator + ## occupation + age_bin ## ## Df Deviance AIC ## + hours_week_bin 2 23116 23194 ## + capital_loss_indicator 1 23270 23346 ## + workclass 6 23426 23512 ## + sex 1 23516 23592 ## + race 4 23527 23609 ## + country_bin 1 23540 23616 ## &lt;none&gt; 23557 23631 ## - age_bin 3 24204 24272 ## - occupation 12 24393 24443 ## - capital_gain_indicator 1 24593 24665 ## - education_level 14 24900 24946 ## - marital_status 6 28013 28075 ## ## Step: AIC=23193.61 ## over_50k ~ marital_status + education_level + capital_gain_indicator + ## occupation + age_bin + hours_week_bin ## ## Df Deviance AIC ## + capital_loss_indicator 1 22847 22927 ## + workclass 6 23013 23103 ## + race 4 23096 23182 ## + country_bin 1 23104 23184 ## + sex 1 23108 23188 ## &lt;none&gt; 23116 23194 ## - hours_week_bin 2 23557 23631 ## - age_bin 3 23728 23800 ## - occupation 12 23802 23856 ## - capital_gain_indicator 1 24120 24196 ## - education_level 14 24340 24390 ## - marital_status 6 27306 27372 ## ## Step: AIC=22927.3 ## over_50k ~ marital_status + education_level + capital_gain_indicator + ## occupation + age_bin + hours_week_bin + capital_loss_indicator ## ## Df Deviance AIC ## + workclass 6 22748 22840 ## + race 4 22828 22916 ## + country_bin 1 22836 22918 ## + sex 1 22840 22922 ## &lt;none&gt; 22847 22927 ## - capital_loss_indicator 1 23116 23194 ## - hours_week_bin 2 23270 23346 ## - age_bin 3 23435 23509 ## - occupation 12 23513 23569 ## - capital_gain_indicator 1 23942 24020 ## - education_level 14 24013 24065 ## - marital_status 6 26986 27054 ## ## Step: AIC=22839.47 ## over_50k ~ marital_status + education_level + capital_gain_indicator + ## occupation + age_bin + hours_week_bin + capital_loss_indicator + ## workclass ## ## Df Deviance AIC ## + race 4 22727 22827 ## + country_bin 1 22736 22830 ## + sex 1 22742 22836 ## &lt;none&gt; 22748 22840 ## - workclass 6 22847 22927 ## - capital_loss_indicator 1 23013 23103 ## - hours_week_bin 2 23145 23233 ## - occupation 12 23327 23395 ## - age_bin 3 23335 23421 ## - capital_gain_indicator 1 23832 23922 ## - education_level 14 23916 23980 ## - marital_status 6 26866 26946 ## ## Step: AIC=22827.35 ## over_50k ~ marital_status + education_level + capital_gain_indicator + ## occupation + age_bin + hours_week_bin + capital_loss_indicator + ## workclass + race ## ## Df Deviance AIC ## + country_bin 1 22717 22819 ## + sex 1 22723 22825 ## &lt;none&gt; 22727 22827 ## - race 4 22748 22840 ## - workclass 6 22828 22916 ## - capital_loss_indicator 1 22992 23090 ## - hours_week_bin 2 23115 23211 ## - occupation 12 23295 23371 ## - age_bin 3 23314 23408 ## - capital_gain_indicator 1 23813 23911 ## - education_level 14 23879 23951 ## - marital_status 6 26765 26853 ## ## Step: AIC=22819.16 ## over_50k ~ marital_status + education_level + capital_gain_indicator + ## occupation + age_bin + hours_week_bin + capital_loss_indicator + ## workclass + race + country_bin ## ## Df Deviance AIC ## + sex 1 22712 22816 ## &lt;none&gt; 22717 22819 ## - country_bin 1 22727 22827 ## - race 4 22736 22830 ## - workclass 6 22818 22908 ## - capital_loss_indicator 1 22980 23080 ## - hours_week_bin 2 23102 23200 ## - occupation 12 23278 23356 ## - age_bin 3 23303 23399 ## - capital_gain_indicator 1 23802 23902 ## - education_level 14 23860 23934 ## - marital_status 6 26761 26851 ## ## Step: AIC=22816.47 ## over_50k ~ marital_status + education_level + capital_gain_indicator + ## occupation + age_bin + hours_week_bin + capital_loss_indicator + ## workclass + race + country_bin + sex ## ## Df Deviance AIC ## &lt;none&gt; 22712 22816 ## - sex 1 22717 22819 ## - country_bin 1 22723 22825 ## - race 4 22730 22826 ## - workclass 6 22812 22904 ## - capital_loss_indicator 1 22975 23077 ## - hours_week_bin 2 23072 23172 ## - occupation 12 23276 23356 ## - age_bin 3 23292 23390 ## - capital_gain_indicator 1 23797 23899 ## - education_level 14 23844 23920 ## - marital_status 6 26022 26114 # Forward selection to select two-variable interactions # First: double check interactions for quasi complete separation table(train$over_50k, train$hours_week_bin, train$occupation) ## , , = ? ## ## ## 0 1 2 ## 0 931 667 175 ## 1 69 69 53 ## ## , , = Adm-clerical ## ## ## 0 1 2 ## 0 1031 1871 426 ## 1 100 313 134 ## ## , , = Craft-repair ## ## ## 0 1 2 ## 0 419 1999 911 ## 1 38 534 408 ## ## , , = Exec-managerial ## ## ## 0 1 2 ## 0 363 1048 848 ## 1 119 662 1237 ## ## , , = Farming-fishing ## ## ## 0 1 2 ## 0 198 313 414 ## 1 6 31 95 ## ## , , = Handlers-cleaners ## ## ## 0 1 2 ## 0 467 812 232 ## 1 8 62 25 ## ## , , = Machine-op-inspct ## ## ## 0 1 2 ## 0 212 1265 345 ## 1 7 165 88 ## ## , , = Other-service ## ## ## 0 1 2 ## 0 1586 1294 394 ## 1 36 55 45 ## ## , , = Prof-specialty ## ## ## 0 1 2 ## 0 661 997 746 ## 1 258 761 943 ## ## , , = Protective-serv ## ## ## 0 1 2 ## 0 82 274 125 ## 1 9 111 93 ## ## , , = Sales ## ## ## 0 1 2 ## 0 1026 955 859 ## 1 72 322 649 ## ## , , = Tech-support ## ## ## 0 1 2 ## 0 171 404 132 ## 1 38 170 81 ## ## , , = Transport-moving ## ## ## 0 1 2 ## 0 208 635 483 ## 1 19 119 206 table(train$over_50k, train$race, train$country_bin) # Quasi complete separation!! ## , , = 0 ## ## ## Amer-Indian-Eskimo Asian-Pac-Islander Black Other White ## 0 12 513 254 129 1840 ## 1 1 212 32 20 441 ## ## , , = 1 ## ## ## Amer-Indian-Eskimo Asian-Pac-Islander Black Other White ## 0 270 224 2607 119 20011 ## 1 38 80 362 12 7012 table(train$over_50k, train$sex, train$capital_gain_indicator) ## , , = 0 ## ## ## Female Male ## 0 9676 15248 ## 1 963 5479 ## ## , , = 1 ## ## ## Female Male ## 0 379 676 ## 1 282 1486 # Build main model with stepwise selected variables main.model &lt;- glm(over_50k ~ marital_status + education_level + capital_gain_indicator + occupation + hours_week_bin + age_bin + capital_loss_indicator + workclass + country_bin + race, data = train, family = binomial(link = &quot;logit&quot;)) # Build model with interactions of interest int.model &lt;- glm(over_50k ~ marital_status + education_level + capital_gain_indicator + occupation + hours_week_bin + age_bin + capital_loss_indicator + workclass + country_bin + race + hours_week_bin*occupation + sex*capital_gain_indicator, data = train, family = binomial(link = &quot;logit&quot;)) # Forward selection for.model &lt;- step(main.model, scope = list(lower=formula(main.model), upper=formula(int.model)), direction = &quot;forward&quot;) ## Start: AIC=22819.16 ## over_50k ~ marital_status + education_level + capital_gain_indicator + ## occupation + hours_week_bin + age_bin + capital_loss_indicator + ## workclass + country_bin + race ## ## Df Deviance AIC ## + occupation:hours_week_bin 24 22654 22804 ## + sex 1 22712 22816 ## &lt;none&gt; 22717 22819 ## ## Step: AIC=22803.48 ## over_50k ~ marital_status + education_level + capital_gain_indicator + ## occupation + hours_week_bin + age_bin + capital_loss_indicator + ## workclass + country_bin + race + occupation:hours_week_bin ## ## Df Deviance AIC ## + sex 1 22648 22800 ## &lt;none&gt; 22654 22804 ## ## Step: AIC=22800.02 ## over_50k ~ marital_status + education_level + capital_gain_indicator + ## occupation + hours_week_bin + age_bin + capital_loss_indicator + ## workclass + country_bin + race + sex + occupation:hours_week_bin ## ## Df Deviance AIC ## &lt;none&gt; 22648 22800 ## + capital_gain_indicator:sex 1 22648 22802 # Final model: # over_50k ~ marital_status + education_level + capital_gain_indicator + # occupation + hours_week_bin + age_bin + capital_loss_indicator + # workclass + country_bin + race + occupation*hours_week_bin #Create Logistic Regression # GLM with binomial logit link logit.model &lt;- glm(over_50k ~ marital_status + education_level + capital_gain_indicator + occupation + hours_week_bin + age_bin + capital_loss_indicator + workclass + country_bin + race + occupation*hours_week_bin, data = train, family = binomial(link = &quot;logit&quot;)) summary(logit.model) ## ## Call: ## glm(formula = over_50k ~ marital_status + education_level + capital_gain_indicator + ## occupation + hours_week_bin + age_bin + capital_loss_indicator + ## workclass + country_bin + race + occupation * hours_week_bin, ## family = binomial(link = &quot;logit&quot;), data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.8041 -0.5131 -0.2063 -0.0514 3.5954 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.113310 1.143211 -4.473 7.72e-06 *** ## marital_statusMarried-AF-spouse 2.644758 0.465985 5.676 1.38e-08 *** ## marital_statusMarried-civ-spouse 2.230477 0.058819 37.921 &lt; 2e-16 *** ## marital_statusMarried-spouse-absent 0.249128 0.198392 1.256 0.20921 ## marital_statusNever-married -0.234255 0.076120 -3.077 0.00209 ** ## marital_statusSeparated -0.056808 0.145784 -0.390 0.69678 ## marital_statusWidowed 0.242581 0.136584 1.776 0.07572 . ## education_level11th 0.206070 0.200128 1.030 0.30316 ## education_level12th 0.407414 0.253873 1.605 0.10854 ## education_level5th-6th -0.163908 0.291235 -0.563 0.57357 ## education_level7th-8th -0.396610 0.221004 -1.795 0.07272 . ## education_level9th -0.316338 0.254529 -1.243 0.21393 ## education_levelAssoc-acdm 1.383717 0.171432 8.072 6.94e-16 *** ## education_levelAssoc-voc 1.275697 0.165877 7.691 1.46e-14 *** ## education_levelBachelors 1.940017 0.153922 12.604 &lt; 2e-16 *** ## education_levelDoctorate 2.881230 0.204936 14.059 &lt; 2e-16 *** ## education_levelEarly-Ed -1.076751 0.495829 -2.172 0.02988 * ## education_levelHS-grad 0.807123 0.150051 5.379 7.49e-08 *** ## education_levelMasters 2.163650 0.163152 13.262 &lt; 2e-16 *** ## education_levelProf-school 2.982543 0.194073 15.368 &lt; 2e-16 *** ## education_levelSome-college 1.132093 0.152333 7.432 1.07e-13 *** ## capital_gain_indicator 1.733954 0.054454 31.842 &lt; 2e-16 *** ## occupationAdm-clerical -0.652211 1.113296 -0.586 0.55798 ## occupationCraft-repair -1.177638 1.123355 -1.048 0.29449 ## occupationExec-managerial -0.304034 1.115092 -0.273 0.78512 ## occupationFarming-fishing -2.124694 1.195362 -1.777 0.07549 . ## occupationHandlers-cleaners -1.989701 1.177188 -1.690 0.09099 . ## occupationMachine-op-inspct -1.965267 1.182867 -1.661 0.09662 . ## occupationOther-service -1.637704 1.122053 -1.460 0.14441 ## occupationProf-specialty -0.307007 1.111718 -0.276 0.78243 ## occupationProtective-serv -1.262652 1.176381 -1.073 0.28312 ## occupationSales -0.953956 1.116804 -0.854 0.39300 ## occupationTech-support -0.202255 1.129106 -0.179 0.85784 ## occupationTransport-moving -1.158173 1.127919 -1.027 0.30450 ## hours_week_bin1 0.807057 0.207184 3.895 9.81e-05 *** ## hours_week_bin2 1.360185 0.252619 5.384 7.27e-08 *** ## age_bin1 0.988553 0.058251 16.971 &lt; 2e-16 *** ## age_bin2 1.383818 0.060353 22.929 &lt; 2e-16 *** ## age_bin3 1.171666 0.070897 16.526 &lt; 2e-16 *** ## capital_loss_indicator 1.122923 0.069762 16.096 &lt; 2e-16 *** ## workclassLocal-gov -0.512325 0.107680 -4.758 1.96e-06 *** ## workclassPrivate -0.397631 0.090469 -4.395 1.11e-05 *** ## workclassSelf-emp-inc -0.125430 0.118158 -1.062 0.28844 ## workclassSelf-emp-not-inc -0.797672 0.106064 -7.521 5.45e-14 *** ## workclassState-gov -0.703359 0.119658 -5.878 4.15e-09 *** ## workclassUnknown -2.003524 1.101525 -1.819 0.06893 . ## country_bin 0.216581 0.067220 3.222 0.00127 ** ## raceAsian-Pac-Islander 0.574802 0.229846 2.501 0.01239 * ## raceBlack 0.318377 0.214327 1.485 0.13742 ## raceOther 0.293562 0.313366 0.937 0.34886 ## raceWhite 0.539509 0.204349 2.640 0.00829 ** ## occupationAdm-clerical:hours_week_bin1 -0.528514 0.251556 -2.101 0.03564 * ## occupationCraft-repair:hours_week_bin1 0.061294 0.284988 0.215 0.82971 ## occupationExec-managerial:hours_week_bin1 -0.255748 0.252714 -1.012 0.31153 ## occupationFarming-fishing:hours_week_bin1 0.209626 0.542153 0.387 0.69901 ## occupationHandlers-cleaners:hours_week_bin1 0.205060 0.471994 0.434 0.66396 ## occupationMachine-op-inspct:hours_week_bin1 0.418074 0.473261 0.883 0.37703 ## occupationOther-service:hours_week_bin1 -0.683593 0.312066 -2.191 0.02848 * ## occupationProf-specialty:hours_week_bin1 -0.181592 0.236361 -0.768 0.44232 ## occupationProtective-serv:hours_week_bin1 0.613777 0.466272 1.316 0.18806 ## occupationSales:hours_week_bin1 0.002308 0.264852 0.009 0.99305 ## occupationTech-support:hours_week_bin1 -0.456773 0.322285 -1.417 0.15640 ## occupationTransport-moving:hours_week_bin1 -0.233238 0.352045 -0.663 0.50764 ## occupationAdm-clerical:hours_week_bin2 -0.723124 0.306719 -2.358 0.01839 * ## occupationCraft-repair:hours_week_bin2 -0.084791 0.322164 -0.263 0.79240 ## occupationExec-managerial:hours_week_bin2 -0.173657 0.289480 -0.600 0.54858 ## occupationFarming-fishing:hours_week_bin2 0.103083 0.534082 0.193 0.84695 ## occupationHandlers-cleaners:hours_week_bin2 -0.179937 0.529191 -0.340 0.73384 ## occupationMachine-op-inspct:hours_week_bin2 0.353019 0.504375 0.700 0.48398 ## occupationOther-service:hours_week_bin2 -0.362031 0.358245 -1.011 0.31222 ## occupationProf-specialty:hours_week_bin2 -0.709664 0.277431 -2.558 0.01053 * ## occupationProtective-serv:hours_week_bin2 0.482623 0.498656 0.968 0.33312 ## occupationSales:hours_week_bin2 -0.006369 0.297781 -0.021 0.98294 ## occupationTech-support:hours_week_bin2 -0.548478 0.376578 -1.456 0.14526 ## occupationTransport-moving:hours_week_bin2 -0.093377 0.376443 -0.248 0.80410 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 37692 on 34188 degrees of freedom ## Residual deviance: 22653 on 34114 degrees of freedom ## AIC: 22803 ## ## Number of Fisher Scoring iterations: 7 10.1 Evaluate logistic regression "],["view-coefficient-of-discrimination-r2.html", "Chapter 11 View Coefficient of Discrimination (R2)", " Chapter 11 View Coefficient of Discrimination (R2) # Get coefficient of discrimination (R2) train$p_hat = predict(logit.model, type = &#39;response&#39;) p1 = train$p_hat[train$over_50k == 1] p0 = train$p_hat[train$over_50k == 0] coef_discrim = mean(p1) - mean(p0) print(coef_discrim) ## [1] 0.4221626 # Coeff of discrimination = 0.422 # Get proportions of non-buy and buy prop0 = 26037/34189 prop1 = 8152/34189 # Plot probabilities as density plot ggplot(train, aes(p_hat, fill = over_50k)) + geom_density(alpha = 0.7) + labs(x = &quot;Predicted Probability&quot;, y = &quot;Density&quot;, fill = &quot;Outcome&quot;, title = paste(&quot;Coefficient of Discrimination = &quot;, round(coef_discrim, 3), sep = &quot;&quot;))+ scale_fill_manual( values = c(&quot;royalblue&quot;,&quot;skyblue&quot;), labels=c(&quot;Not Over 50k&quot;, &quot;Over 50k&quot;))+ theme_classic() "],["determine-optimal-cut-off.html", "Chapter 12 Determine optimal cut-off", " Chapter 12 Determine optimal cut-off # Iterate through cut-off values to determine optimal cut-off train$p_hat &lt;- predict(logit.model, type = &quot;response&quot;) youden &lt;- NULL cutoff &lt;- NULL for(i in 1:49){ cutoff = c(cutoff, i/50) youden &lt;- c(youden, youdensIndex(train$over_50k, train$p_hat, threshold = i/50)) } # Print table with lowest Youdens at the top of the list ctable &lt;- data.frame(cutoff, youden) print(ctable[order(-youden),]) ## cutoff youden ## 11 0.22 0.633206967 ## 12 0.24 0.633205195 ## 10 0.20 0.630218964 ## 13 0.26 0.628882318 ## 9 0.18 0.627749252 ## 14 0.28 0.623915742 ## 8 0.16 0.622297720 ## 15 0.30 0.617430953 ## 7 0.14 0.614189110 ## 16 0.32 0.612148278 ## 17 0.34 0.602534568 ## 6 0.12 0.599212012 ## 18 0.36 0.589834237 ## 19 0.38 0.582557902 ## 5 0.10 0.577505574 ## 20 0.40 0.574083058 ## 21 0.42 0.565781160 ## 4 0.08 0.550827261 ## 22 0.44 0.549960206 ## 23 0.46 0.534836814 ## 24 0.48 0.523700197 ## 25 0.50 0.513396682 ## 3 0.06 0.509912457 ## 26 0.52 0.499713771 ## 27 0.54 0.482961039 ## 28 0.56 0.474162341 ## 29 0.58 0.458158311 ## 30 0.60 0.447456132 ## 2 0.04 0.446203691 ## 31 0.62 0.430246725 ## 32 0.64 0.414261669 ## 33 0.66 0.395898027 ## 34 0.68 0.372572642 ## 35 0.70 0.353401199 ## 36 0.72 0.332082124 ## 1 0.02 0.329923583 ## 37 0.74 0.311871380 ## 38 0.76 0.283551542 ## 39 0.78 0.263821683 ## 40 0.80 0.235130112 ## 41 0.82 0.209636491 ## 42 0.84 0.192418040 ## 43 0.86 0.161744371 ## 44 0.88 0.136807533 ## 45 0.90 0.116525040 ## 46 0.92 0.095504863 ## 47 0.94 0.068759959 ## 48 0.96 0.036399483 ## 49 0.98 0.007147865 # Confusion matrix for train using Youden&#39;s Index optimal cut off train$classification = ifelse(train$p_hat &gt;= 0.2, 1, 0) confusionMatrix(train$over_50k, factor(train$classification)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 19676 6303 ## 1 1044 7166 ## ## Accuracy : 0.7851 ## 95% CI : (0.7807, 0.7895) ## No Information Rate : 0.606 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.517 ## ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Sensitivity : 0.9496 ## Specificity : 0.5320 ## Pos Pred Value : 0.7574 ## Neg Pred Value : 0.8728 ## Prevalence : 0.6060 ## Detection Rate : 0.5755 ## Detection Prevalence : 0.7599 ## Balanced Accuracy : 0.7408 ## ## &#39;Positive&#39; Class : 0 ## "],["auroc.html", "Chapter 13 AUROC", " Chapter 13 AUROC # Evaluate model using AUROC train$p_hat &lt;- predict(logit.model, type = &quot;response&quot;) plotROC(train$over_50k, train$p_hat) # 0.902 AUROC "],["test-training-cut-off-on-validation.html", "Chapter 14 Test training cut off on validation 14.1 Create XGBoost", " Chapter 14 Test training cut off on validation # Make predictions on validation set validation$p_hat = predict(logit.model, newdata = validation, type = &#39;response&#39;) # Confusion Matrix using Youdens cutoff validation$classification = ifelse(validation$p_hat &gt;= 0.2, 1, 0) confusionMatrix(validation$over_50k, factor(validation$classification)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 2781 867 ## 1 140 1096 ## ## Accuracy : 0.7938 ## 95% CI : (0.7822, 0.8051) ## No Information Rate : 0.5981 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.5434 ## ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Sensitivity : 0.9521 ## Specificity : 0.5583 ## Pos Pred Value : 0.7623 ## Neg Pred Value : 0.8867 ## Prevalence : 0.5981 ## Detection Rate : 0.5694 ## Detection Prevalence : 0.7469 ## Balanced Accuracy : 0.7552 ## ## &#39;Positive&#39; Class : 0 ## # Get concordance Concordance(validation$over_50k, validation$p_hat) ## $Concordance ## [1] 0.9080194 ## ## $Discordance ## [1] 0.09198062 ## ## $Tied ## [1] 2.775558e-17 ## ## $Pairs ## [1] 4508928 # Evaluate model using AUROC plotROC(validation$over_50k, validation$p_hat) # 0.899 AUROC 14.1 Create XGBoost "],["tuning-an-xgboost-nrounds-parameter---11-was-lowest.html", "Chapter 15 Tuning an XGBoost nrounds parameter - 11 was lowest", " Chapter 15 Tuning an XGBoost nrounds parameter - 11 was lowest xgbcv &lt;- xgb.cv(data = train_x, label = train_y, subsample = 0.5, nrounds = 100, nfold = 10) ## [1] train-rmse:0.426840+0.000232 test-rmse:0.427665+0.001046 ## [2] train-rmse:0.384976+0.000432 test-rmse:0.386772+0.001776 ## [3] train-rmse:0.360679+0.000437 test-rmse:0.363177+0.002416 ## [4] train-rmse:0.346871+0.000389 test-rmse:0.350217+0.002977 ## [5] train-rmse:0.338734+0.000533 test-rmse:0.342838+0.003107 ## [6] train-rmse:0.333629+0.000644 test-rmse:0.338377+0.003073 ## [7] train-rmse:0.330259+0.000658 test-rmse:0.335733+0.003201 ## [8] train-rmse:0.327933+0.000552 test-rmse:0.333956+0.003372 ## [9] train-rmse:0.326167+0.000531 test-rmse:0.332774+0.003391 ## [10] train-rmse:0.324766+0.000464 test-rmse:0.331928+0.003512 ## [11] train-rmse:0.323627+0.000540 test-rmse:0.331301+0.003705 ## [12] train-rmse:0.322550+0.000492 test-rmse:0.330725+0.003750 ## [13] train-rmse:0.321755+0.000592 test-rmse:0.330306+0.003595 ## [14] train-rmse:0.321052+0.000560 test-rmse:0.329969+0.003717 ## [15] train-rmse:0.320408+0.000533 test-rmse:0.329763+0.003688 ## [16] train-rmse:0.319800+0.000527 test-rmse:0.329478+0.003830 ## [17] train-rmse:0.319255+0.000526 test-rmse:0.329329+0.003811 ## [18] train-rmse:0.318685+0.000524 test-rmse:0.329212+0.003779 ## [19] train-rmse:0.318145+0.000561 test-rmse:0.329060+0.003889 ## [20] train-rmse:0.317699+0.000574 test-rmse:0.328972+0.003837 ## [21] train-rmse:0.317193+0.000592 test-rmse:0.328868+0.003736 ## [22] train-rmse:0.316739+0.000598 test-rmse:0.328735+0.003772 ## [23] train-rmse:0.316317+0.000622 test-rmse:0.328825+0.003711 ## [24] train-rmse:0.315821+0.000558 test-rmse:0.328636+0.003893 ## [25] train-rmse:0.315466+0.000566 test-rmse:0.328692+0.003921 ## [26] train-rmse:0.315099+0.000585 test-rmse:0.328802+0.003921 ## [27] train-rmse:0.314731+0.000581 test-rmse:0.328724+0.004030 ## [28] train-rmse:0.314425+0.000607 test-rmse:0.328849+0.003944 ## [29] train-rmse:0.314086+0.000627 test-rmse:0.328978+0.003958 ## [30] train-rmse:0.313740+0.000599 test-rmse:0.328968+0.003921 ## [31] train-rmse:0.313386+0.000558 test-rmse:0.329079+0.003938 ## [32] train-rmse:0.313079+0.000511 test-rmse:0.329087+0.004007 ## [33] train-rmse:0.312777+0.000540 test-rmse:0.329146+0.003950 ## [34] train-rmse:0.312540+0.000526 test-rmse:0.329225+0.004006 ## [35] train-rmse:0.312222+0.000540 test-rmse:0.329378+0.003934 ## [36] train-rmse:0.311916+0.000531 test-rmse:0.329511+0.003921 ## [37] train-rmse:0.311646+0.000550 test-rmse:0.329633+0.003869 ## [38] train-rmse:0.311363+0.000558 test-rmse:0.329543+0.003799 ## [39] train-rmse:0.311122+0.000576 test-rmse:0.329627+0.003816 ## [40] train-rmse:0.310848+0.000544 test-rmse:0.329724+0.003792 ## [41] train-rmse:0.310597+0.000554 test-rmse:0.329746+0.003827 ## [42] train-rmse:0.310347+0.000570 test-rmse:0.329719+0.003760 ## [43] train-rmse:0.310070+0.000557 test-rmse:0.329798+0.003714 ## [44] train-rmse:0.309825+0.000515 test-rmse:0.329859+0.003721 ## [45] train-rmse:0.309575+0.000533 test-rmse:0.329846+0.003761 ## [46] train-rmse:0.309304+0.000523 test-rmse:0.329927+0.003758 ## [47] train-rmse:0.309099+0.000570 test-rmse:0.330027+0.003832 ## [48] train-rmse:0.308863+0.000558 test-rmse:0.330117+0.003760 ## [49] train-rmse:0.308646+0.000576 test-rmse:0.330215+0.003706 ## [50] train-rmse:0.308420+0.000595 test-rmse:0.330305+0.003614 ## [51] train-rmse:0.308181+0.000628 test-rmse:0.330435+0.003679 ## [52] train-rmse:0.307930+0.000630 test-rmse:0.330536+0.003804 ## [53] train-rmse:0.307715+0.000618 test-rmse:0.330612+0.003800 ## [54] train-rmse:0.307524+0.000576 test-rmse:0.330678+0.003831 ## [55] train-rmse:0.307321+0.000601 test-rmse:0.330798+0.003766 ## [56] train-rmse:0.307089+0.000629 test-rmse:0.330875+0.003751 ## [57] train-rmse:0.306909+0.000626 test-rmse:0.330974+0.003783 ## [58] train-rmse:0.306700+0.000628 test-rmse:0.331054+0.003777 ## [59] train-rmse:0.306487+0.000653 test-rmse:0.331138+0.003767 ## [60] train-rmse:0.306266+0.000648 test-rmse:0.331266+0.003811 ## [61] train-rmse:0.306051+0.000656 test-rmse:0.331396+0.003792 ## [62] train-rmse:0.305848+0.000644 test-rmse:0.331498+0.003953 ## [63] train-rmse:0.305648+0.000655 test-rmse:0.331642+0.004034 ## [64] train-rmse:0.305458+0.000692 test-rmse:0.331634+0.004021 ## [65] train-rmse:0.305284+0.000675 test-rmse:0.331690+0.004069 ## [66] train-rmse:0.305105+0.000676 test-rmse:0.331891+0.003969 ## [67] train-rmse:0.304961+0.000708 test-rmse:0.331910+0.003974 ## [68] train-rmse:0.304762+0.000698 test-rmse:0.332028+0.004098 ## [69] train-rmse:0.304579+0.000692 test-rmse:0.332084+0.003954 ## [70] train-rmse:0.304417+0.000679 test-rmse:0.332185+0.004031 ## [71] train-rmse:0.304231+0.000695 test-rmse:0.332158+0.003977 ## [72] train-rmse:0.304073+0.000704 test-rmse:0.332192+0.003933 ## [73] train-rmse:0.303909+0.000702 test-rmse:0.332316+0.003815 ## [74] train-rmse:0.303727+0.000699 test-rmse:0.332455+0.003829 ## [75] train-rmse:0.303544+0.000675 test-rmse:0.332600+0.003784 ## [76] train-rmse:0.303374+0.000681 test-rmse:0.332764+0.003764 ## [77] train-rmse:0.303202+0.000659 test-rmse:0.332919+0.003756 ## [78] train-rmse:0.303041+0.000655 test-rmse:0.333017+0.003747 ## [79] train-rmse:0.302869+0.000661 test-rmse:0.333059+0.003707 ## [80] train-rmse:0.302694+0.000687 test-rmse:0.333186+0.003636 ## [81] train-rmse:0.302529+0.000695 test-rmse:0.333153+0.003580 ## [82] train-rmse:0.302363+0.000701 test-rmse:0.333163+0.003610 ## [83] train-rmse:0.302234+0.000682 test-rmse:0.333335+0.003591 ## [84] train-rmse:0.302094+0.000675 test-rmse:0.333487+0.003533 ## [85] train-rmse:0.301959+0.000674 test-rmse:0.333567+0.003545 ## [86] train-rmse:0.301796+0.000682 test-rmse:0.333579+0.003587 ## [87] train-rmse:0.301667+0.000692 test-rmse:0.333679+0.003641 ## [88] train-rmse:0.301499+0.000682 test-rmse:0.333683+0.003611 ## [89] train-rmse:0.301335+0.000694 test-rmse:0.333764+0.003702 ## [90] train-rmse:0.301166+0.000699 test-rmse:0.333808+0.003735 ## [91] train-rmse:0.301030+0.000721 test-rmse:0.333905+0.003797 ## [92] train-rmse:0.300870+0.000726 test-rmse:0.333992+0.003836 ## [93] train-rmse:0.300731+0.000729 test-rmse:0.334053+0.003792 ## [94] train-rmse:0.300573+0.000770 test-rmse:0.334130+0.003768 ## [95] train-rmse:0.300432+0.000778 test-rmse:0.334112+0.003810 ## [96] train-rmse:0.300291+0.000787 test-rmse:0.334160+0.003856 ## [97] train-rmse:0.300132+0.000798 test-rmse:0.334290+0.003864 ## [98] train-rmse:0.300011+0.000812 test-rmse:0.334264+0.003900 ## [99] train-rmse:0.299882+0.000804 test-rmse:0.334285+0.003967 ## [100] train-rmse:0.299747+0.000807 test-rmse:0.334416+0.003967 eval &lt;- xgbcv$evaluation_log eval %&gt;% arrange(test_rmse_mean) ## iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std ## 1: 24 0.3158214 0.0005578276 0.3286356 0.003893069 ## 2: 25 0.3154657 0.0005663125 0.3286916 0.003920634 ## 3: 27 0.3147308 0.0005806319 0.3287242 0.004030272 ## 4: 22 0.3167393 0.0005978209 0.3287351 0.003771819 ## 5: 26 0.3150990 0.0005849877 0.3288019 0.003920608 ## 6: 23 0.3163171 0.0006218415 0.3288247 0.003710851 ## 7: 28 0.3144252 0.0006071443 0.3288491 0.003943560 ## 8: 21 0.3171933 0.0005916027 0.3288679 0.003735844 ## 9: 30 0.3137397 0.0005991604 0.3289679 0.003921463 ## 10: 20 0.3176994 0.0005743599 0.3289716 0.003837461 ## 11: 29 0.3140862 0.0006265959 0.3289781 0.003957684 ## 12: 19 0.3181450 0.0005605991 0.3290604 0.003888687 ## 13: 31 0.3133860 0.0005584955 0.3290791 0.003937651 ## 14: 32 0.3130792 0.0005113830 0.3290871 0.004006965 ## 15: 33 0.3127771 0.0005401854 0.3291459 0.003950444 ## 16: 18 0.3186845 0.0005239140 0.3292124 0.003779094 ## 17: 34 0.3125404 0.0005256345 0.3292252 0.004005743 ## 18: 17 0.3192552 0.0005262560 0.3293291 0.003811094 ## 19: 35 0.3122218 0.0005403171 0.3293784 0.003933540 ## 20: 16 0.3198003 0.0005266101 0.3294776 0.003830202 ## 21: 36 0.3119158 0.0005310514 0.3295107 0.003920797 ## 22: 38 0.3113634 0.0005583349 0.3295434 0.003798873 ## 23: 39 0.3111218 0.0005758246 0.3296273 0.003816108 ## 24: 37 0.3116455 0.0005495617 0.3296334 0.003868884 ## 25: 42 0.3103465 0.0005698523 0.3297194 0.003759505 ## 26: 40 0.3108476 0.0005438460 0.3297243 0.003791991 ## 27: 41 0.3105969 0.0005541003 0.3297455 0.003827358 ## 28: 15 0.3204085 0.0005330616 0.3297627 0.003687844 ## 29: 43 0.3100699 0.0005565397 0.3297985 0.003713987 ## 30: 45 0.3095754 0.0005331666 0.3298465 0.003760805 ## 31: 44 0.3098246 0.0005151804 0.3298591 0.003721093 ## 32: 46 0.3093040 0.0005234625 0.3299267 0.003758041 ## 33: 14 0.3210524 0.0005603092 0.3299690 0.003717462 ## 34: 47 0.3090993 0.0005704078 0.3300267 0.003832022 ## 35: 48 0.3088633 0.0005584577 0.3301173 0.003760161 ## 36: 49 0.3086459 0.0005759664 0.3302148 0.003706137 ## 37: 50 0.3084196 0.0005945827 0.3303054 0.003614100 ## 38: 13 0.3217546 0.0005920190 0.3303058 0.003595123 ## 39: 51 0.3081808 0.0006284460 0.3304347 0.003678696 ## 40: 52 0.3079299 0.0006298505 0.3305359 0.003803991 ## 41: 53 0.3077150 0.0006183528 0.3306117 0.003800470 ## 42: 54 0.3075245 0.0005764891 0.3306777 0.003831229 ## 43: 12 0.3225500 0.0004915783 0.3307252 0.003750003 ## 44: 55 0.3073208 0.0006012064 0.3307982 0.003766038 ## 45: 56 0.3070886 0.0006294145 0.3308745 0.003751045 ## 46: 57 0.3069094 0.0006258337 0.3309742 0.003782962 ## 47: 58 0.3066996 0.0006277095 0.3310541 0.003777318 ## 48: 59 0.3064866 0.0006530179 0.3311382 0.003766507 ## 49: 60 0.3062665 0.0006479854 0.3312659 0.003811496 ## 50: 11 0.3236271 0.0005404334 0.3313014 0.003704549 ## 51: 61 0.3060507 0.0006564573 0.3313964 0.003792334 ## 52: 62 0.3058477 0.0006438935 0.3314976 0.003953418 ## 53: 64 0.3054581 0.0006917045 0.3316335 0.004020777 ## 54: 63 0.3056478 0.0006553388 0.3316418 0.004033924 ## 55: 65 0.3052844 0.0006751255 0.3316903 0.004068940 ## 56: 66 0.3051053 0.0006760982 0.3318910 0.003968789 ## 57: 67 0.3049608 0.0007082388 0.3319095 0.003974047 ## 58: 10 0.3247662 0.0004644282 0.3319284 0.003512389 ## 59: 68 0.3047616 0.0006981832 0.3320276 0.004097559 ## 60: 69 0.3045787 0.0006916378 0.3320843 0.003953591 ## 61: 71 0.3042306 0.0006945572 0.3321576 0.003976826 ## 62: 70 0.3044171 0.0006787283 0.3321851 0.004031304 ## 63: 72 0.3040735 0.0007039728 0.3321918 0.003933245 ## 64: 73 0.3039087 0.0007018367 0.3323161 0.003815136 ## 65: 74 0.3037270 0.0006994924 0.3324548 0.003828768 ## 66: 75 0.3035436 0.0006746433 0.3325996 0.003783710 ## 67: 76 0.3033738 0.0006812286 0.3327642 0.003763842 ## 68: 9 0.3261674 0.0005309993 0.3327741 0.003391027 ## 69: 77 0.3032020 0.0006593052 0.3329188 0.003756018 ## 70: 78 0.3030407 0.0006552671 0.3330168 0.003746821 ## 71: 79 0.3028690 0.0006607770 0.3330588 0.003707347 ## 72: 81 0.3025286 0.0006950272 0.3331531 0.003580067 ## 73: 82 0.3023634 0.0007014425 0.3331625 0.003610021 ## 74: 80 0.3026944 0.0006870584 0.3331863 0.003636336 ## 75: 83 0.3022343 0.0006819232 0.3333346 0.003590593 ## 76: 84 0.3020942 0.0006747052 0.3334875 0.003532634 ## 77: 85 0.3019587 0.0006737415 0.3335670 0.003545193 ## 78: 86 0.3017960 0.0006823158 0.3335788 0.003587117 ## 79: 87 0.3016670 0.0006916148 0.3336793 0.003640635 ## 80: 88 0.3014986 0.0006821733 0.3336829 0.003611296 ## 81: 89 0.3013353 0.0006942681 0.3337639 0.003702295 ## 82: 90 0.3011662 0.0006985048 0.3338077 0.003735383 ## 83: 91 0.3010305 0.0007208092 0.3339047 0.003797331 ## 84: 8 0.3279327 0.0005516633 0.3339555 0.003372492 ## 85: 92 0.3008702 0.0007258761 0.3339920 0.003836450 ## 86: 93 0.3007315 0.0007289753 0.3340534 0.003792374 ## 87: 95 0.3004316 0.0007779526 0.3341118 0.003809868 ## 88: 94 0.3005725 0.0007699855 0.3341296 0.003768355 ## 89: 96 0.3002910 0.0007867574 0.3341601 0.003856486 ## 90: 98 0.3000108 0.0008115618 0.3342637 0.003899586 ## 91: 99 0.2998825 0.0008044693 0.3342846 0.003966632 ## 92: 97 0.3001317 0.0007980942 0.3342898 0.003863663 ## 93: 100 0.2997468 0.0008074521 0.3344156 0.003967305 ## 94: 7 0.3302589 0.0006583722 0.3357334 0.003200781 ## 95: 6 0.3336293 0.0006442116 0.3383775 0.003072875 ## 96: 5 0.3387344 0.0005330062 0.3428380 0.003106588 ## 97: 4 0.3468706 0.0003885829 0.3502168 0.002976906 ## 98: 3 0.3606789 0.0004372163 0.3631770 0.002415856 ## 99: 2 0.3849757 0.0004321081 0.3867724 0.001775833 ## 100: 1 0.4268404 0.0002318436 0.4276651 0.001046021 ## iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std "],["tuning-through-caret.html", "Chapter 16 Tuning through caret", " Chapter 16 Tuning through caret eta = .9 and subsample = .25, max tree depth = 10 "],["variable-importance.html", "Chapter 17 Variable importance", " Chapter 17 Variable importance xgb &lt;- xgboost(data = train_x, label = train_y, subsample = .25, nrounds = 11, eta = 0.9, max_depth = 10) ## [1] train-rmse:0.342728 ## [2] train-rmse:0.338387 ## [3] train-rmse:0.339324 ## [4] train-rmse:0.340820 ## [5] train-rmse:0.342915 ## [6] train-rmse:0.344458 ## [7] train-rmse:0.344395 ## [8] train-rmse:0.345190 ## [9] train-rmse:0.345926 ## [10] train-rmse:0.345981 ## [11] train-rmse:0.345230 xgb.importance(feature_names = colnames(train_x), model = xgb) ## Feature Gain Cover Frequency ## 1: marital_statusMarried-civ-spouse 0.202268852 0.0519293656 0.056933842 ## 2: capital_gain_indicator 0.078395162 0.0508894162 0.054707379 ## 3: education_levelBachelors 0.040241700 0.0366025437 0.028307888 ## 4: hours_week_bin2 0.038085858 0.0404228983 0.042938931 ## 5: age_bin2 0.036806225 0.0514450648 0.040394402 ## 6: education_levelMasters 0.033892330 0.0305801339 0.020674300 ## 7: age_bin1 0.032224269 0.0143538963 0.055979644 ## 8: hours_week_bin1 0.031400032 0.0157722057 0.072837150 ## 9: occupationProf-specialty 0.028181312 0.0217989395 0.022264631 ## 10: sexMale 0.028044680 0.0118448292 0.040076336 ## 11: occupationExec-managerial 0.027970869 0.0355852959 0.024491094 ## 12: capital_loss_indicator 0.025734948 0.0511456199 0.034351145 ## 13: age_bin3 0.023024186 0.0257565848 0.030852417 ## 14: workclassPrivate 0.022718286 0.0138317595 0.034351145 ## 15: country_bin 0.021526396 0.0256787508 0.027035623 ## 16: workclassSelf-emp-not-inc 0.018706378 0.0428962915 0.019402036 ## 17: education_levelSome-college 0.017925398 0.0193460859 0.023218830 ## 18: education_levelHS-grad 0.017679529 0.0143149793 0.021628499 ## 19: occupationCraft-repair 0.017331588 0.0262495338 0.018765903 ## 20: marital_statusNever-married 0.016379987 0.0044160014 0.026399491 ## 21: workclassState-gov 0.016083814 0.0147884698 0.017175573 ## 22: workclassLocal-gov 0.014411399 0.0338297056 0.020038168 ## 23: raceBlack 0.013964939 0.0055521623 0.020992366 ## 24: occupationSales 0.013078476 0.0136252831 0.017811705 ## 25: raceAsian-Pac-Islander 0.013009837 0.0163559611 0.016539440 ## 26: education_levelDoctorate 0.012210119 0.0565237367 0.013040712 ## 27: education_levelAssoc-voc 0.011658715 0.0215492219 0.014312977 ## 28: occupationAdm-clerical 0.011005503 0.0064277954 0.011768448 ## 29: occupationTech-support 0.010573191 0.0120318471 0.011132316 ## 30: occupationProtective-serv 0.009985635 0.0336740375 0.011132316 ## 31: education_levelAssoc-acdm 0.009958910 0.0205708911 0.010814249 ## 32: workclassSelf-emp-inc 0.009895760 0.0225318768 0.014949109 ## 33: raceWhite 0.009522610 0.0057597198 0.011132316 ## 34: occupationTransport-moving 0.009472242 0.0037911669 0.009860051 ## 35: occupationOther-service 0.009266543 0.0101778832 0.009223919 ## 36: education_levelProf-school 0.008341388 0.0238410023 0.010178117 ## 37: education_level7th-8th 0.007373421 0.0121756239 0.005725191 ## 38: workclassUnknown 0.006960324 0.0086276884 0.008587786 ## 39: marital_statusSeparated 0.005908569 0.0043922188 0.009860051 ## 40: marital_statusWidowed 0.005388384 0.0041922285 0.009860051 ## 41: occupationFarming-fishing 0.005242445 0.0056894529 0.006997455 ## 42: occupationHandlers-cleaners 0.004996354 0.0158684172 0.006361323 ## 43: occupationMachine-op-inspct 0.004397261 0.0012658844 0.006997455 ## 44: marital_statusMarried-spouse-absent 0.004272120 0.0031674135 0.007633588 ## 45: education_level9th 0.003855729 0.0042095249 0.004452926 ## 46: education_level11th 0.002511712 0.0079639369 0.006361323 ## 47: marital_statusMarried-AF-spouse 0.002209353 0.0250690507 0.002862595 ## 48: education_level5th-6th 0.001729555 0.0072374857 0.002226463 ## 49: raceOther 0.001550917 0.0003394429 0.002862595 ## 50: education_levelEarly-Ed 0.001433336 0.0057002632 0.001590331 ## 51: education_level12th 0.001193451 0.0082104114 0.001908397 ## Feature Gain Cover Frequency xgb.ggplot.importance(xgb.importance(feature_names = colnames(train_x), model = xgb)) 17.0.1 ROC Curve and AUC # get predictions # Prepare data for predict function validation_x &lt;- model.matrix(over_50k ~ ., data = validation)[, 3:53] validation_y &lt;- as.numeric(as.character(validation$over_50k)) p_hat_xgb &lt;- predict(xgb, newdata = validation_x, type = &quot;response&quot;) # create ROC object rocobj &lt;- roc(validation$over_50k, p_hat_xgb) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases rocobj$auc # to get AUC - 0.816 ## Area under the curve: 0.831 # create ROC plot with minimal theme ggroc(rocobj, colour = &#39;steelblue&#39;, size = 2) + ggtitle(paste0(&#39;XGBoost Model ROC Curve &#39;, &#39;(AUC = &#39;, round(rocobj$auc, 4), &#39;)&#39;)) + theme_minimal()+ labs(x=&quot;1-Specificity (FPR)&quot;,y=&quot;Sensitivity (TPR)&quot;)+geom_abline(intercept = 1.00, size = 0.5, linetype=&quot;dotted&quot;, color = &quot;red&quot;)+ coord_cartesian(xlim=c(1,0)) # Interesting relationship # Add color blind palette cbPalette = c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;) # Plot data ggplot(data = train) + geom_bar(mapping = aes(x = over_50k, fill = marital_status, color =)) + scale_fill_manual(values=cbPalette) + #scale_fill_discrete(labels = c(&quot;Divorced&quot;, &quot;Married- Armed Forces&quot;, &quot;Married- Civil Spouse&quot;, &quot;Married- Spouse Absent&quot;, &quot;Never Married&quot;, &quot;Separated&quot;, &quot;Widowed&quot;)) + labs(title=&quot;Marital Status over Earning Category&quot;, x =&quot;Earning Over 50k (0: No, 1: Yes)&quot;, y = &quot;Frequency&quot;, fill = &quot;Marital Status&quot;) + theme_minimal() # Get final AUROC on test data # Make predictions on validation set test$p_hat = predict(logit.model, newdata = test, type = &#39;response&#39;) # Confusion Matrix using Youdens cutoff test$classification = ifelse(test$p_hat &gt;= 0.2, 1, 0) confusionMatrix(test$over_50k, factor(test$classification)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 5683 1845 ## 1 273 1968 ## ## Accuracy : 0.7832 ## 95% CI : (0.7749, 0.7913) ## No Information Rate : 0.6097 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.508 ## ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Sensitivity : 0.9542 ## Specificity : 0.5161 ## Pos Pred Value : 0.7549 ## Neg Pred Value : 0.8782 ## Prevalence : 0.6097 ## Detection Rate : 0.5817 ## Detection Prevalence : 0.7706 ## Balanced Accuracy : 0.7351 ## ## &#39;Positive&#39; Class : 0 ## # Evaluate model using AUROC table(census$over_50k) ## ## 0 1 ## 37155 11687 plotROC(test$over_50k, test$p_hat) # 0.894 AUROC "],["conclusion-write-up.html", "Chapter 18 Conclusion / Write Up", " Chapter 18 Conclusion / Write Up Overview The goal of this project is to develop a model that predicts whether individuals, based on census variables, make over $50,000/year. I compared the predictive power of a logistic regression to an XGBoost model using the area under the ROC curve (AUROC). This statistic calculates each models ability to distinguish between people who make above $50,000/year from those who do not. The logistic regression model resulted in the higher AUROC, and was able to better predict who is likely to make above $50,000/year from census data. The final AUROC on the test data was 0.894. Methodology The first two steps were exploring each variable individually and binning continuous variables into categories. Each variable was then tested for quasi-complete separation. This is a problem when a subgroup of the predictor variable perfectly predicts the target variable outcome. Each subgroup with less than 5 observations was collapsed into a new, larger group. The data was split into a 70/20/10 training/validation/test split and each variable was tested for multi-collinearity to ensure no two variables are providing the same information in the model. Stepwise selection was used to identify which variables were important in predicting the target. The 13 important variables were then used in a forward selection process to identify any predictive interactions. For example, the selection process identified changes in the target variables relationship with hours worked each week when looking at different occupations. A logistic regression was built using these variables. The XGBoost was tuned and built using all census variables, and the most important variables were calculated using a gain statistic. Results A simple relationship between marital status and people who make over 50k can be pictured in Figure 1. This figure shows the large majority of people who make over 50k are married to civil spouses. In our census data, 85% of people who make over 50k are married to a civil spouse compared to only 33% of those who do not make over 50k. Furthermore, the XGBoost model found marital status to be the most important variable in predicting the target. The logistic regression was able to better predict people who make over $50,000/year using census variables. I recommend using a logistic regression moving forward to best predict the individuals that make over 50K using census data. Given more time for tuning, you might be able to challenge the predictive power using a future XGBoost model. This repo was initially generated from a bookdown template available here: https://github.com/jtr13/bookdown-template. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
