[["index.html", "RTI Take Home Assignment Chapter 1 SQL Data Consolidation", " RTI Take Home Assignment Louise Lindegaard 2022-02-07 Chapter 1 SQL Data Consolidation Main objective: Select all variables from the records table and join them with all other tables to consolidate 9 tables into 1 â€“ The select statement contains 2 data cleaning steps: 1. Remove redundant columns 2. Rename variables to logical names -- The select statement contains 2 data cleaning steps: 1. Remove redundant columns 2. Rename variables to logical names SELECT r.id as id, over_50k, age, education_num, capital_gain, capital_loss, hours_week, workclass_id, w.name as workclass, education_level_id, e.name as education_level, marital_status_id, m.name as marital_status, occupation_id, o.name as occupation, relationship_id, rel.name as relationship, race_id, races.name as race, sex_id, s.name as sex, country_id, c.name as country FROM records as r -- The left join includes all observations from the records table LEFT JOIN countries as c ON r.country_id = c.id LEFT JOIN education_levels as e ON r.education_level_id = e.id LEFT JOIN marital_statuses as m ON r.marital_status_id = m.id LEFT JOIN occupations as o ON r.occupation_id = o.id LEFT JOIN races ON r.race_id = races.id LEFT JOIN relationships as rel ON r.relationship_id = rel.id LEFT JOIN sexes as s ON r.sex_id = s.id LEFT JOIN workclasses as w ON r.workclass_id = w.id; -- Per instruction, the above statement was exported to CSV "],["variable-cleaning-and-formating.html", "Chapter 2 Variable Cleaning and Formating 2.1 Import data and libraries into R 2.2 Data Cleaning 2.3 Variable binning 2.4 Check for Quasi Complete Separation 2.5 Split data into training, validation and test 2.6 Exploratory Statistics on Training Data", " Chapter 2 Variable Cleaning and Formating 2.1 Import data and libraries into R census = read.csv(&#39;exercise01_consolidated.csv&#39;) library(ggplot2) library(InformationValue) library(gmodels) library(DescTools) library(vcd) library(vcdExtra) library(stats) library(mgcv) library(car) library(dplyr) library(ROCR) library(caret) library(mgcv) library(xgboost) library(optbin) library(pROC) 2.2 Data Cleaning #Exploring data types for (x in 1:23) { print(colnames(census[x])) print(class(census[,x])) } # Set ordinal/ nominal variables as a factor (less than 20 levels) col_names &lt;- sapply(census, function(col) length(unique(col)) &lt; 20) census[ , col_names] &lt;- lapply(census[ , col_names] , factor) ## Check for NA Values (represented with &quot;?&quot;) in each variable sapply(census, function(x) any(x == &quot;?&quot;)) # NA values are only in categorical variables - will explore binning options below ## Data looks very clean, ready to start exploring 2.3 Variable binning ## Visualize data and explore each variable hist(census$age) # mostly normal with slight skew hist(census$capital_gain) # Zero Inflated hist(census$capital_loss) # Zero Inflated hist(census$hours_week) # Heavily inflated at 40 hours table(census$education_level) # Lots of categories, check for quasi-complete separation later ## Explore zero inflated variables colSums(census==0)/nrow(census)*100 # 91.7 % Capital Gain = 0 and 95.3 % capital loss = 0 # Bin to binary variable: census$capital_gain_indicator = ifelse(census$capital_gain == 0, 0, 1) census$capital_loss_indicator = ifelse(census$capital_loss == 0, 0, 1) # Bin the working hours variable based on inflation at 40 hours census$hours_week_bin[census$hours_week &lt; 40] = 0 census$hours_week_bin[census$hours_week &gt; 40] = 2 census$hours_week_bin[census$hours_week == 40] = 1 # Factorize census$hours_week_bin = factor(census$hours_week_bin) ## Explore country variable table(census$country) # 90% in US, will create binary variable instead for simplicity census$country_bin = ifelse(census$country == &quot;United-States&quot;, 1, 0) # Explore only normal continuous variable for linear relationship gam.age &lt;- gam(over_50k ~ s(age), data = census, family = binomial(link = &#39;logit&#39;), method = &#39;REML&#39;) summary(gam.age) plot(gam.age) # age has non-linear relationship with logit- bin this variable using optbin optbin(census$age, numbin = 4) # Upper inclusive limits of ideal splits is 29, 41, 55, 90 census$age_bin[census$age &lt;= 29] = 0 census$age_bin[census$age &gt; 29 &amp; census$age &lt;= 41] = 1 census$age_bin[census$age &gt; 41 &amp; census$age &lt;= 55] = 2 census$age_bin[census$age &gt; 55] = 3 census$age_bin = factor(census$age_bin) # Remove redundant variables (including education number and relationship to householder) census = census[,c(&#39;id&#39;, &#39;over_50k&#39;, &#39;age_bin&#39;, &#39;capital_gain_indicator&#39;, &#39;capital_loss_indicator&#39;, &#39;hours_week_bin&#39;, &#39;workclass&#39;, &#39;education_level&#39;, &#39;marital_status&#39;, &#39;occupation&#39;, &#39;race&#39;, &#39;sex&#39;, &#39;country_bin&#39; )] 2.4 Check for Quasi Complete Separation # This loop will print out the column name for any cross table that contains zeros (separation) or &lt; 5 observations for (i in 1:13) { x = table(census[, i], census$over_50k) for (k in 1:length(x)) { if (x[k] &lt;= 5) { print(c(names(census)[i], names(census$over_50k))) break } } } # Workclass contains quasi complete separation CrossTable(census$workclass, census$over_50k) # Collapse the never worked and without pay categories with missing as this is the most similar split census = census %&gt;% mutate(workclass = as.character(workclass), workclass = if_else(workclass == &#39;Never-worked&#39; | workclass == &#39;Without-pay&#39; | workclass == &#39;?&#39;, &#39;Unknown&#39;, workclass), workclass = factor(workclass)) # Collapse education levels containing &lt;5 observations: pre-school and 1-4th grade census = census %&gt;% mutate(education_level = as.character(education_level), education_level = if_else(education_level == &#39;Preschool&#39; | education_level == &#39;1st-4th&#39;, &#39;Early-Ed&#39;, education_level), education_level = factor(education_level)) CrossTable(census$occupation, census$over_50k) # Collapse armed-forces with Protective-serv, and Priv-house-serv with Handlers-cleaners census = census %&gt;% mutate(occupation = as.character(occupation), occupation = if_else(occupation == &#39;Armed-Forces&#39;, &#39;Protective-serv&#39;, occupation), occupation = if_else(occupation == &#39;Priv-house-serv&#39;, &#39;Handlers-cleaners&#39;, occupation), occupation = factor(occupation)) CrossTable(census$occupation, census$over_50k) 2.5 Split data into training, validation and test #Split the data into a 70/20/10 training, validation, and test data split. train = census %&gt;% sample_frac(0.7) test_validation = anti_join(census, train, by = &#39;id&#39;) test = test_validation %&gt;% sample_frac(2/3) validation = anti_join(test_validation, test, by = &#39;id&#39;) 2.6 Exploratory Statistics on Training Data prop.table(table(train$over_50k)) # 24 % over 50k # Explore odds ratio for binary variables OddsRatio(table(train$over_50k, train$capital_gain_indicator)) #Those with non-zero capital gains are 6.31 times as likely to make over 50k than those without OddsRatio(table(train$over_50k, train$capital_loss_indicator)) #&#39;*Those with non-zero capital losses are 3.36 times as likely to make over 50k than those without* OddsRatio(table(train$over_50k, train$country_bin)) #Those in the US are 1.35 times as likely to make above 50k compared to those out of the US OddsRatio(table(train$over_50k, train$sex)) #Men are 3.52 times as likely to make above 50k compared to women OddsRatio(table(train$capital_gain_indicator, train$sex)) #&#39;*Men are 1.72 times as likely to have non-zero capital gains than women* #Test association for nominal variables using Chi-square test chisq.test(table(train$over_50k, train$workclass)) # p-value &lt; 2.2e-16 chisq.test(table(train$over_50k, train$marital_status)) # p-value &lt; 2.2e-16 chisq.test(table(train$over_50k, train$occupation)) # p-value &lt; 2.2e-16 chisq.test(table(train$over_50k, train$race)) # p-value &lt; 2.2e-16 chisq.test(table(train$over_50k, train$hours_week_bin)) # p-value &lt; 2.2e-16 chisq.test(table(train$over_50k, train$education_level)) # p-value &lt; 2.2e-16 "],["develop-logistic-regression-model.html", "Chapter 3 Develop Logistic Regression Model 3.1 Check for issues with multi-collinearity 3.2 Variable Selection 3.3 Create Logistic Regression 3.4 Evaluate logistic regression 3.5 Test training cut off on validation", " Chapter 3 Develop Logistic Regression Model 3.1 Check for issues with multi-collinearity VIF.model=glm(over_50k~.,data=train[,2:13],family = binomial(link = &quot;logit&quot;)) VIF.model # No VIF over 10 3.2 Variable Selection # Stepwise selection to select relevant variables full.model &lt;- glm(over_50k ~ ., data=train[,2:13], family = binomial(link = &quot;logit&quot;)) empty.model &lt;- glm(over_50k ~ 1, data=train[,2:13], family = binomial(link = &quot;logit&quot;)) step.model &lt;- step(empty.model, scope = list(lower=formula(empty.model), upper=formula(full.model)), direction = &quot;both&quot;) # Forward selection to select two-variable interactions # First: double check interactions for quasi complete separation table(train$over_50k, train$hours_week_bin, train$occupation) table(train$over_50k, train$race, train$country_bin) # Quasi complete separation!! table(train$over_50k, train$sex, train$capital_gain_indicator) # Build main model with stepwise selected variables main.model &lt;- glm(over_50k ~ marital_status + education_level + capital_gain_indicator + occupation + hours_week_bin + age_bin + capital_loss_indicator + workclass + country_bin + race, data = train, family = binomial(link = &quot;logit&quot;)) # Build model with interactions of interest int.model &lt;- glm(over_50k ~ marital_status + education_level + capital_gain_indicator + occupation + hours_week_bin + age_bin + capital_loss_indicator + workclass + country_bin + race + hours_week_bin*occupation + sex*capital_gain_indicator, data = train, family = binomial(link = &quot;logit&quot;)) # Forward selection for.model &lt;- step(main.model, scope = list(lower=formula(main.model), upper=formula(int.model)), direction = &quot;forward&quot;) # Final model: # over_50k ~ marital_status + education_level + capital_gain_indicator + # occupation + hours_week_bin + age_bin + capital_loss_indicator + # workclass + country_bin + race + occupation*hours_week_bin 3.3 Create Logistic Regression # GLM with binomial logit link logit.model &lt;- glm(over_50k ~ marital_status + education_level + capital_gain_indicator + occupation + hours_week_bin + age_bin + capital_loss_indicator + workclass + country_bin + race + occupation*hours_week_bin, data = train, family = binomial(link = &quot;logit&quot;)) summary(logit.model) ## ## Call: ## glm(formula = over_50k ~ marital_status + education_level + capital_gain_indicator + ## occupation + hours_week_bin + age_bin + capital_loss_indicator + ## workclass + country_bin + race + occupation * hours_week_bin, ## family = binomial(link = &quot;logit&quot;), data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.7685 -0.5126 -0.2043 -0.0527 3.5227 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.5019233 1.1384240 -4.833 1.35e-06 *** ## marital_statusMarried-AF-spouse 2.4567360 0.4942194 4.971 6.66e-07 *** ## marital_statusMarried-civ-spouse 2.2708142 0.0594819 38.177 &lt; 2e-16 *** ## marital_statusMarried-spouse-absent 0.2331385 0.1998541 1.167 0.24339 ## marital_statusNever-married -0.1372397 0.0760122 -1.805 0.07100 . ## marital_statusSeparated -0.0771906 0.1485700 -0.520 0.60337 ## marital_statusWidowed 0.2162612 0.1353538 1.598 0.11010 ## education_level11th -0.0004525 0.2058609 -0.002 0.99825 ## education_level12th 0.5455755 0.2499226 2.183 0.02904 * ## education_level5th-6th -0.3950432 0.3018824 -1.309 0.19067 ## education_level7th-8th -0.4335397 0.2205075 -1.966 0.04929 * ## education_level9th -0.5958271 0.2637695 -2.259 0.02389 * ## education_levelAssoc-acdm 1.3238604 0.1694649 7.812 5.63e-15 *** ## education_levelAssoc-voc 1.2286830 0.1631694 7.530 5.07e-14 *** ## education_levelBachelors 1.8703162 0.1516060 12.337 &lt; 2e-16 *** ## education_levelDoctorate 2.8596888 0.2020710 14.152 &lt; 2e-16 *** ## education_levelEarly-Ed -0.7827920 0.4337737 -1.805 0.07114 . ## education_levelHS-grad 0.7399556 0.1476078 5.013 5.36e-07 *** ## education_levelMasters 2.1514321 0.1608346 13.377 &lt; 2e-16 *** ## education_levelProf-school 2.8771576 0.1923141 14.961 &lt; 2e-16 *** ## education_levelSome-college 1.0993339 0.1498169 7.338 2.17e-13 *** ## capital_gain_indicator 1.7019206 0.0545081 31.223 &lt; 2e-16 *** ## occupationAdm-clerical -0.4395121 1.1063638 -0.397 0.69118 ## occupationCraft-repair -0.9521341 1.1190072 -0.851 0.39484 ## occupationExec-managerial 0.1092478 1.1102976 0.098 0.92162 ## occupationFarming-fishing -1.8008076 1.1903892 -1.513 0.13033 ## occupationHandlers-cleaners -2.0582334 1.1906457 -1.729 0.08387 . ## occupationMachine-op-inspct -1.6468696 1.1804767 -1.395 0.16299 ## occupationOther-service -1.2219740 1.1172313 -1.094 0.27406 ## occupationProf-specialty 0.1149432 1.1069977 0.104 0.91730 ## occupationProtective-serv -0.8317505 1.1761105 -0.707 0.47944 ## occupationSales -0.5020593 1.1114258 -0.452 0.65147 ## occupationTech-support 0.2339071 1.1235800 0.208 0.83509 ## occupationTransport-moving -0.7927409 1.1214803 -0.707 0.47965 ## hours_week_bin1 0.6383133 0.2134081 2.991 0.00278 ** ## hours_week_bin2 1.1151911 0.2514373 4.435 9.20e-06 *** ## age_bin1 1.0250604 0.0593304 17.277 &lt; 2e-16 *** ## age_bin2 1.4673777 0.0612681 23.950 &lt; 2e-16 *** ## age_bin3 1.2226766 0.0716979 17.053 &lt; 2e-16 *** ## capital_loss_indicator 1.1125858 0.0689056 16.147 &lt; 2e-16 *** ## workclassLocal-gov -0.6171600 0.1077242 -5.729 1.01e-08 *** ## workclassPrivate -0.3564311 0.0899520 -3.962 7.42e-05 *** ## workclassSelf-emp-inc -0.0972363 0.1174541 -0.828 0.40775 ## workclassSelf-emp-not-inc -0.7577894 0.1056415 -7.173 7.33e-13 *** ## workclassState-gov -0.6857089 0.1187059 -5.777 7.63e-09 *** ## workclassUnknown -1.5940910 1.0969196 -1.453 0.14616 ## country_bin 0.1749891 0.0669126 2.615 0.00892 ** ## raceAsian-Pac-Islander 0.4928537 0.2323218 2.121 0.03389 * ## raceBlack 0.3985577 0.2169832 1.837 0.06624 . ## raceOther 0.3174704 0.3202712 0.991 0.32156 ## raceWhite 0.5613457 0.2072047 2.709 0.00675 ** ## occupationAdm-clerical:hours_week_bin1 -0.2498372 0.2583998 -0.967 0.33361 ## occupationCraft-repair:hours_week_bin1 0.2712757 0.2898670 0.936 0.34934 ## occupationExec-managerial:hours_week_bin1 -0.1100426 0.2567848 -0.429 0.66826 ## occupationFarming-fishing:hours_week_bin1 0.2560892 0.5495599 0.466 0.64122 ## occupationHandlers-cleaners:hours_week_bin1 0.7817547 0.5157618 1.516 0.12959 ## occupationMachine-op-inspct:hours_week_bin1 0.5767818 0.4803223 1.201 0.22982 ## occupationOther-service:hours_week_bin1 -0.4916005 0.3139800 -1.566 0.11742 ## occupationProf-specialty:hours_week_bin1 -0.1151010 0.2415777 -0.476 0.63375 ## occupationProtective-serv:hours_week_bin1 0.8114712 0.4790100 1.694 0.09025 . ## occupationSales:hours_week_bin1 0.1271090 0.2661690 0.478 0.63297 ## occupationTech-support:hours_week_bin1 -0.4213875 0.3232001 -1.304 0.19230 ## occupationTransport-moving:hours_week_bin1 -0.0160218 0.3578882 -0.045 0.96429 ## occupationAdm-clerical:hours_week_bin2 -0.2592373 0.3069274 -0.845 0.39832 ## occupationCraft-repair:hours_week_bin2 0.2757611 0.3214072 0.858 0.39090 ## occupationExec-managerial:hours_week_bin2 0.0080143 0.2874915 0.028 0.97776 ## occupationFarming-fishing:hours_week_bin2 0.2870610 0.5323542 0.539 0.58973 ## occupationHandlers-cleaners:hours_week_bin2 0.5294664 0.5662377 0.935 0.34976 ## occupationMachine-op-inspct:hours_week_bin2 0.5132940 0.5100220 1.006 0.31422 ## occupationOther-service:hours_week_bin2 -0.1400107 0.3542101 -0.395 0.69264 ## occupationProf-specialty:hours_week_bin2 -0.4323419 0.2761633 -1.566 0.11746 ## occupationProtective-serv:hours_week_bin2 0.7534618 0.5082167 1.483 0.13819 ## occupationSales:hours_week_bin2 0.0678859 0.2936788 0.231 0.81719 ## occupationTech-support:hours_week_bin2 -0.4202828 0.3735739 -1.125 0.26058 ## occupationTransport-moving:hours_week_bin2 0.0705081 0.3779149 0.187 0.85200 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 37679 on 34188 degrees of freedom ## Residual deviance: 22580 on 34114 degrees of freedom ## AIC: 22730 ## ## Number of Fisher Scoring iterations: 7 3.4 Evaluate logistic regression View Coefficient of Discrimination (R2) # Get coefficient of discrimination (R2) train$p_hat = predict(logit.model, type = &#39;response&#39;) p1 = train$p_hat[train$over_50k == 1] p0 = train$p_hat[train$over_50k == 0] coef_discrim = mean(p1) - mean(p0) print(coef_discrim) ## [1] 0.4233827 # Coeff of discrimination = 0.422 # Get proportions of non-buy and buy prop0 = 26037/34189 prop1 = 8152/34189 # Plot probabilities as density plot ggplot(train, aes(p_hat, fill = over_50k)) + geom_density(alpha = 0.7) + labs(x = &quot;Predicted Probability&quot;, y = &quot;Density&quot;, fill = &quot;Outcome&quot;, title = paste(&quot;Coefficient of Discrimination = &quot;, round(coef_discrim, 3), sep = &quot;&quot;))+ scale_fill_manual( values = c(&quot;royalblue&quot;,&quot;skyblue&quot;), labels=c(&quot;Not Over 50k&quot;, &quot;Over 50k&quot;))+ theme_classic() Determine optimal cut-off # Iterate through cut-off values to determine optimal cut-off train$p_hat &lt;- predict(logit.model, type = &quot;response&quot;) youden &lt;- NULL cutoff &lt;- NULL for(i in 1:49){ cutoff = c(cutoff, i/50) youden &lt;- c(youden, youdensIndex(train$over_50k, train$p_hat, threshold = i/50)) } # Print table with lowest Youdens at the top of the list ctable &lt;- data.frame(cutoff, youden) print(ctable[order(-youden),]) ## cutoff youden ## 11 0.22 0.636907648 ## 12 0.24 0.634563071 ## 10 0.20 0.632811920 ## 9 0.18 0.629858622 ## 13 0.26 0.627834918 ## 14 0.28 0.625946184 ## 8 0.16 0.624775517 ## 15 0.30 0.618867789 ## 7 0.14 0.617397226 ## 16 0.32 0.612263761 ## 6 0.12 0.604514644 ## 17 0.34 0.600199727 ## 18 0.36 0.592768218 ## 19 0.38 0.585042518 ## 5 0.10 0.580066980 ## 20 0.40 0.579869786 ## 21 0.42 0.565503328 ## 4 0.08 0.556498639 ## 22 0.44 0.551214316 ## 23 0.46 0.538330838 ## 24 0.48 0.528796561 ## 25 0.50 0.513994699 ## 3 0.06 0.512360659 ## 26 0.52 0.501252593 ## 27 0.54 0.484359727 ## 28 0.56 0.474665235 ## 29 0.58 0.461865643 ## 2 0.04 0.450244332 ## 30 0.60 0.444767478 ## 31 0.62 0.429793156 ## 32 0.64 0.414247700 ## 33 0.66 0.397887644 ## 34 0.68 0.369370456 ## 35 0.70 0.352253607 ## 36 0.72 0.333308381 ## 1 0.02 0.332575530 ## 37 0.74 0.309070445 ## 38 0.76 0.284069115 ## 39 0.78 0.264142474 ## 40 0.80 0.235702413 ## 41 0.82 0.210162311 ## 42 0.84 0.191518993 ## 43 0.86 0.165639137 ## 44 0.88 0.140285332 ## 45 0.90 0.117568381 ## 46 0.92 0.090617262 ## 47 0.94 0.064897941 ## 48 0.96 0.033943715 ## 49 0.98 0.007313506 # Confusion matrix for train using Youden&#39;s Index optimal cut off train$classification = ifelse(train$p_hat &gt;= 0.2, 1, 0) confusionMatrix(train$over_50k, factor(train$classification)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 19725 6260 ## 1 1036 7168 ## ## Accuracy : 0.7866 ## 95% CI : (0.7822, 0.7909) ## No Information Rate : 0.6072 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.5196 ## ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Sensitivity : 0.9501 ## Specificity : 0.5338 ## Pos Pred Value : 0.7591 ## Neg Pred Value : 0.8737 ## Prevalence : 0.6072 ## Detection Rate : 0.5769 ## Detection Prevalence : 0.7600 ## Balanced Accuracy : 0.7420 ## ## &#39;Positive&#39; Class : 0 ## AUROC # Evaluate model using AUROC train$p_hat &lt;- predict(logit.model, type = &quot;response&quot;) plotROC(train$over_50k, train$p_hat) # 0.902 AUROC 3.5 Test training cut off on validation # Make predictions on validation set validation$p_hat = predict(logit.model, newdata = validation, type = &#39;response&#39;) # Confusion Matrix using Youdens cutoff validation$classification = ifelse(validation$p_hat &gt;= 0.2, 1, 0) confusionMatrix(validation$over_50k, factor(validation$classification)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 2882 865 ## 1 135 1002 ## ## Accuracy : 0.7952 ## 95% CI : (0.7837, 0.8065) ## No Information Rate : 0.6177 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.5316 ## ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Sensitivity : 0.9553 ## Specificity : 0.5367 ## Pos Pred Value : 0.7691 ## Neg Pred Value : 0.8813 ## Prevalence : 0.6177 ## Detection Rate : 0.5901 ## Detection Prevalence : 0.7672 ## Balanced Accuracy : 0.7460 ## ## &#39;Positive&#39; Class : 0 ## # Get concordance Concordance(validation$over_50k, validation$p_hat) ## $Concordance ## [1] 0.9118859 ## ## $Discordance ## [1] 0.08811411 ## ## $Tied ## [1] -1.387779e-17 ## ## $Pairs ## [1] 4260339 # Evaluate model using AUROC plotROC(validation$over_50k, validation$p_hat) # 0.899 AUROC "],["create-xgboost.html", "Chapter 4 Create XGBoost 4.1 Tuning an XGBoost nrounds parameter - 11 was lowest 4.2 Tuning through caret 4.3 Variable importance 4.4 ROC Curve and AUC", " Chapter 4 Create XGBoost #XGBoost model here # Prepare data for XGBoost function train_x &lt;- model.matrix(over_50k ~ ., data = train)[, 3:53] train_y &lt;- as.numeric(as.character(train$over_50k)) 4.1 Tuning an XGBoost nrounds parameter - 11 was lowest xgbcv &lt;- xgb.cv(data = train_x, label = train_y, subsample = 0.5, nrounds = 100, nfold = 10) eval &lt;- xgbcv$evaluation_log eval %&gt;% arrange(test_rmse_mean) 4.2 Tuning through caret tune_grid &lt;- expand.grid( nrounds = 11, eta = c(0.8, 0.85, 0.9, 0.95), max_depth = c(5:15), gamma = c(0), colsample_bytree = 1, min_child_weight = 1, subsample = c(0.2, 0.25, 0.3) ) Tune XGBoost, but this takes too long to render xgb.caret &lt;- train(x = train_x, y = as.factor(train_y), method = &quot;xgbTree&quot;, tuneGrid = tune_grid, trControl = trainControl(method = &#39;cv&#39;, # Using 10-fold cross-validation number = 10)) plot(xgb.caret) xgb.caret eta = .9 and subsample = .25, max tree depth = 10 4.3 Variable importance xgb &lt;- xgboost(data = train_x, label = train_y, subsample = .25, nrounds = 11, eta = 0.9, max_depth = 10) ## [1] train-rmse:0.341861 ## [2] train-rmse:0.336858 ## [3] train-rmse:0.337494 ## [4] train-rmse:0.338419 ## [5] train-rmse:0.339444 ## [6] train-rmse:0.340151 ## [7] train-rmse:0.340251 ## [8] train-rmse:0.340741 ## [9] train-rmse:0.340683 ## [10] train-rmse:0.341842 ## [11] train-rmse:0.343642 xgb.importance(feature_names = colnames(train_x), model = xgb) ## Feature Gain Cover Frequency ## 1: marital_statusMarried-civ-spouse 0.2170486945 0.0406630705 0.058394161 ## 2: capital_gain_indicator 0.0509703637 0.0587049263 0.044747699 ## 3: education_levelBachelors 0.0498892447 0.0445254917 0.027927642 ## 4: occupationProf-specialty 0.0402544496 0.0388857970 0.019358934 ## 5: hours_week_bin2 0.0377129450 0.0338693860 0.047921295 ## 6: occupationExec-managerial 0.0371504773 0.0460174972 0.022215170 ## 7: age_bin2 0.0349664232 0.0345917061 0.051412250 ## 8: capital_loss_indicator 0.0318790597 0.0355842235 0.039035227 ## 9: hours_week_bin1 0.0311808658 0.0103493080 0.057759441 ## 10: sexMale 0.0310402415 0.0104343501 0.039669946 ## 11: age_bin1 0.0309987916 0.0274255585 0.060615678 ## 12: education_levelMasters 0.0295998742 0.0367274485 0.018724215 ## 13: age_bin3 0.0222435099 0.0167296230 0.039669946 ## 14: workclassLocal-gov 0.0218294472 0.0215232020 0.026340844 ## 15: workclassPrivate 0.0215192540 0.0156617518 0.034274833 ## 16: education_levelSome-college 0.0208081911 0.0159524022 0.024754046 ## 17: marital_statusNever-married 0.0198628925 0.0111405230 0.030783878 ## 18: education_levelHS-grad 0.0155813780 0.0195510850 0.027610282 ## 19: country_bin 0.0150838248 0.0136067457 0.026658204 ## 20: raceWhite 0.0146382639 0.0124872033 0.017454776 ## 21: raceAsian-Pac-Islander 0.0143401568 0.0143753546 0.016820057 ## 22: workclassSelf-emp-not-inc 0.0143334631 0.0238968471 0.017772136 ## 23: education_levelAssoc-voc 0.0140624851 0.0228795706 0.017772136 ## 24: workclassSelf-emp-inc 0.0133687868 0.0504978196 0.013646461 ## 25: occupationSales 0.0130195780 0.0203498355 0.017454776 ## 26: occupationCraft-repair 0.0118867741 0.0167403878 0.015233259 ## 27: education_levelDoctorate 0.0117016143 0.0394380328 0.011424944 ## 28: raceBlack 0.0112355082 0.0133774548 0.018724215 ## 29: workclassState-gov 0.0105171380 0.0301953494 0.013646461 ## 30: occupationTransport-moving 0.0099517709 0.0100780343 0.009520787 ## 31: education_levelProf-school 0.0095963753 0.0149232844 0.006029832 ## 32: occupationFarming-fishing 0.0082661150 0.0171279217 0.006029832 ## 33: occupationOther-service 0.0078412236 0.0157177289 0.007616630 ## 34: education_levelAssoc-acdm 0.0076647336 0.0171462219 0.013329102 ## 35: occupationAdm-clerical 0.0070971234 0.0122568359 0.010472866 ## 36: occupationTech-support 0.0068552793 0.0131255578 0.008886068 ## 37: occupationProtective-serv 0.0064961303 0.0434177906 0.006981911 ## 38: education_level7th-8th 0.0055750859 0.0064782750 0.007299270 ## 39: workclassUnknown 0.0054032885 0.0011690606 0.006029832 ## 40: education_level11th 0.0049837833 0.0049938048 0.006981911 ## 41: marital_statusWidowed 0.0048798903 0.0035459351 0.010472866 ## 42: occupationMachine-op-inspct 0.0045711844 0.0130555864 0.005395113 ## 43: marital_statusMarried-spouse-absent 0.0044131857 0.0025986301 0.006347191 ## 44: marital_statusSeparated 0.0043003708 0.0031960782 0.008251349 ## 45: education_level12th 0.0032412191 0.0110651692 0.004443034 ## 46: raceOther 0.0022337056 0.0023747216 0.004125674 ## 47: occupationHandlers-cleaners 0.0021477871 0.0081177586 0.002856236 ## 48: education_level9th 0.0019087187 0.0058054731 0.003808315 ## 49: education_level5th-6th 0.0018359684 0.0055546525 0.003490955 ## 50: marital_statusMarried-AF-spouse 0.0012898087 0.0118962141 0.002221517 ## 51: education_levelEarly-Ed 0.0007235556 0.0001733138 0.001586798 ## Feature Gain Cover Frequency xgb.ggplot.importance(xgb.importance(feature_names = colnames(train_x), model = xgb)) 4.4 ROC Curve and AUC # get predictions # Prepare data for predict function validation_x &lt;- model.matrix(over_50k ~ ., data = validation)[, 3:53] validation_y &lt;- as.numeric(as.character(validation$over_50k)) p_hat_xgb &lt;- predict(xgb, newdata = validation_x, type = &quot;response&quot;) # create ROC object rocobj &lt;- roc(validation$over_50k, p_hat_xgb) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases rocobj$auc # to get AUC - 0.816 ## Area under the curve: 0.847 # create ROC plot with minimal theme ggroc(rocobj, colour = &#39;steelblue&#39;, size = 2) + ggtitle(paste0(&#39;XGBoost Model ROC Curve &#39;, &#39;(AUC = &#39;, round(rocobj$auc, 4), &#39;)&#39;)) + theme_minimal()+ labs(x=&quot;1-Specificity (FPR)&quot;,y=&quot;Sensitivity (TPR)&quot;)+geom_abline(intercept = 1.00, size = 0.5, linetype=&quot;dotted&quot;, color = &quot;red&quot;)+ coord_cartesian(xlim=c(1,0)) "],["conclusion-write-up.html", "Chapter 5 Conclusion / Write Up 5.1 Figure 1 Code 5.2 Final AUROC Chart and Code", " Chapter 5 Conclusion / Write Up Overview The goal of this project is to develop a model that predicts whether individuals, based on census variables, make over $50,000/year. I compared the predictive power of a logistic regression to an XGBoost model using the area under the ROC curve (AUROC). This statistic calculates each models ability to distinguish between people who make above $50,000/year from those who do not. The logistic regression model resulted in the higher AUROC, and was able to better predict who is likely to make above $50,000/year from census data. The final AUROC on the test data was 0.894. Methodology The first two steps were exploring each variable individually and binning continuous variables into categories. Each variable was then tested for quasi-complete separation. This is a problem when a subgroup of the predictor variable perfectly predicts the target variable outcome. Each subgroup with less than 5 observations was collapsed into a new, larger group. The data was split into a 70/20/10 training/validation/test split and each variable was tested for multi-collinearity to ensure no two variables are providing the same information in the model. Stepwise selection was used to identify which variables were important in predicting the target. The 13 important variables were then used in a forward selection process to identify any predictive interactions. For example, the selection process identified changes in the target variables relationship with hours worked each week when looking at different occupations. A logistic regression was built using these variables. The XGBoost was tuned and built using all census variables, and the most important variables were calculated using a gain statistic. Results A simple relationship between marital status and people who make over 50k can be pictured in Figure 1 below. This figure shows the large majority of people who make over 50k are married to civil spouses. In our census data, 85% of people who make over 50k are married to a civil spouse compared to only 33% of those who do not make over 50k. Furthermore, the XGBoost model found marital status to be the most important variable in predicting the target. Figure 1. Marital status for those who make more than $50,000 (1) and those who do not (0) The logistic regression was able to better predict people who make over $50,000/year using census variables. I recommend using a logistic regression moving forward to best predict the individuals that make over 50K using census data. Given more time for tuning, you might be able to challenge the predictive power using a future XGBoost model. 5.1 Figure 1 Code # Add color blind palette cbPalette = c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;) # Plot data ggplot(data = train) + geom_bar(mapping = aes(x = over_50k, fill = marital_status, color =)) + scale_fill_manual(values=cbPalette) + #scale_fill_discrete(labels = c(&quot;Divorced&quot;, &quot;Married- Armed Forces&quot;, &quot;Married- Civil Spouse&quot;, &quot;Married- Spouse Absent&quot;, &quot;Never Married&quot;, &quot;Separated&quot;, &quot;Widowed&quot;)) + labs(title=&quot;Marital Status over Earning Category&quot;, x =&quot;Earning Over 50k (0: No, 1: Yes)&quot;, y = &quot;Frequency&quot;, fill = &quot;Marital Status&quot;) + theme_minimal() 5.2 Final AUROC Chart and Code # Make predictions on validation set test$p_hat = predict(logit.model, newdata = test, type = &#39;response&#39;) # Evaluate model using AUROC plotROC(test$over_50k, test$p_hat) # 0.894 AUROC This repo was initially generated from a bookdown template available here: https://github.com/jtr13/bookdown-template. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
