[["index.html", "RTI Take Home Assignment Chapter 1 SQL Data Consolidation", " RTI Take Home Assignment Louise Lindegaard 2022-02-07 Chapter 1 SQL Data Consolidation Main objective: Select all variables from the records table and join them with all other tables to consolidate 9 tables into 1 â€“ The select statement contains 2 data cleaning steps: 1. Remove redundant columns 2. Rename variables to logical names -- The select statement contains 2 data cleaning steps: 1. Remove redundant columns 2. Rename variables to logical names SELECT r.id as id, over_50k, age, education_num, capital_gain, capital_loss, hours_week, workclass_id, w.name as workclass, education_level_id, e.name as education_level, marital_status_id, m.name as marital_status, occupation_id, o.name as occupation, relationship_id, rel.name as relationship, race_id, races.name as race, sex_id, s.name as sex, country_id, c.name as country FROM records as r -- The left join includes all observations from the records table LEFT JOIN countries as c ON r.country_id = c.id LEFT JOIN education_levels as e ON r.education_level_id = e.id LEFT JOIN marital_statuses as m ON r.marital_status_id = m.id LEFT JOIN occupations as o ON r.occupation_id = o.id LEFT JOIN races ON r.race_id = races.id LEFT JOIN relationships as rel ON r.relationship_id = rel.id LEFT JOIN sexes as s ON r.sex_id = s.id LEFT JOIN workclasses as w ON r.workclass_id = w.id; -- Per instruction, the above statement was exported to CSV "],["variable-cleaning-and-formating.html", "Chapter 2 Variable Cleaning and Formating 2.1 Import data and libraries into R 2.2 Data Cleaning 2.3 Variable binning 2.4 Check for Quasi Complete Separation 2.5 Split data into training, validation and test 2.6 Exploratory Statistics on Training Data", " Chapter 2 Variable Cleaning and Formating 2.1 Import data and libraries into R 2.2 Data Cleaning 2.3 Variable binning 2.4 Check for Quasi Complete Separation 2.5 Split data into training, validation and test 2.6 Exploratory Statistics on Training Data "],["develop-logistic-regression-model.html", "Chapter 3 Develop Logistic Regression Model 3.1 Check for issues with multi-collinearity 3.2 Variable Selection 3.3 Create Logistic Regression 3.4 Evaluate logistic regression 3.5 Test training cut off on validation", " Chapter 3 Develop Logistic Regression Model 3.1 Check for issues with multi-collinearity 3.2 Variable Selection # Stepwise selection to select relevant variables full.model &lt;- glm(over_50k ~ ., data=train[,2:13], family = binomial(link = &quot;logit&quot;)) empty.model &lt;- glm(over_50k ~ 1, data=train[,2:13], family = binomial(link = &quot;logit&quot;)) step.model &lt;- step(empty.model, scope = list(lower=formula(empty.model), upper=formula(full.model)), direction = &quot;both&quot;) ## Start: AIC=37728.96 ## over_50k ~ 1 ## ## Df Deviance AIC ## + marital_status 6 30156 30170 ## + occupation 12 33403 33429 ## + education_level 14 33399 33429 ## + age_bin 3 33903 33911 ## + hours_week_bin 2 35272 35278 ## + capital_gain_indicator 1 35611 35615 ## + sex 1 36010 36014 ## + workclass 6 36646 36660 ## + capital_loss_indicator 1 37175 37179 ## + race 4 37358 37368 ## + country_bin 1 37673 37677 ## &lt;none&gt; 37727 37729 ## ## Step: AIC=30169.7 ## over_50k ~ marital_status ## ## Df Deviance AIC ## + education_level 14 26141 26183 ## + occupation 12 26728 26766 ## + capital_gain_indicator 1 28613 28629 ## + age_bin 3 28878 28898 ## + hours_week_bin 2 29058 29076 ## + workclass 6 29561 29587 ## + capital_loss_indicator 1 29785 29801 ## + race 4 30060 30082 ## + country_bin 1 30082 30098 ## + sex 1 30109 30125 ## &lt;none&gt; 30156 30170 ## - marital_status 6 37727 37729 ## ## Step: AIC=26183.25 ## over_50k ~ marital_status + education_level ## ## Df Deviance AIC ## + capital_gain_indicator 1 25014 25058 ## + occupation 12 25141 25207 ## + age_bin 3 25238 25286 ## + hours_week_bin 2 25457 25503 ## + workclass 6 25786 25840 ## + capital_loss_indicator 1 25902 25946 ## + country_bin 1 26079 26123 ## + sex 1 26082 26126 ## + race 4 26101 26151 ## &lt;none&gt; 26141 26183 ## - education_level 14 30156 30170 ## - marital_status 6 33399 33429 ## ## Step: AIC=25058.19 ## over_50k ~ marital_status + education_level + capital_gain_indicator ## ## Df Deviance AIC ## + occupation 12 24064 24132 ## + age_bin 3 24176 24226 ## + hours_week_bin 2 24356 24404 ## + capital_loss_indicator 1 24665 24711 ## + workclass 6 24678 24734 ## + sex 1 24958 25004 ## + country_bin 1 24960 25006 ## + race 4 24978 25030 ## &lt;none&gt; 25014 25058 ## - capital_gain_indicator 1 26141 26183 ## - education_level 14 28613 28629 ## - marital_status 6 31854 31886 ## ## Step: AIC=24132.32 ## over_50k ~ marital_status + education_level + capital_gain_indicator + ## occupation ## ## Df Deviance AIC ## + age_bin 3 23327 23401 ## + hours_week_bin 2 23587 23659 ## + capital_loss_indicator 1 23743 23813 ## + workclass 6 23900 23980 ## + sex 1 24024 24094 ## + country_bin 1 24031 24101 ## + race 4 24044 24120 ## &lt;none&gt; 24064 24132 ## - occupation 12 25014 25058 ## - capital_gain_indicator 1 25141 25207 ## - education_level 14 25488 25528 ## - marital_status 6 30177 30233 ## ## Step: AIC=23401.41 ## over_50k ~ marital_status + education_level + capital_gain_indicator + ## occupation + age_bin ## ## Df Deviance AIC ## + hours_week_bin 2 22887 22965 ## + capital_loss_indicator 1 23026 23102 ## + workclass 6 23162 23248 ## + country_bin 1 23298 23374 ## + sex 1 23301 23377 ## + race 4 23305 23387 ## &lt;none&gt; 23327 23401 ## - age_bin 3 24064 24132 ## - occupation 12 24176 24226 ## - capital_gain_indicator 1 24341 24413 ## - education_level 14 24622 24668 ## - marital_status 6 27832 27894 ## ## Step: AIC=22965.21 ## over_50k ~ marital_status + education_level + capital_gain_indicator + ## occupation + age_bin + hours_week_bin ## ## Df Deviance AIC ## + capital_loss_indicator 1 22598 22678 ## + workclass 6 22756 22846 ## + country_bin 1 22863 22943 ## + race 4 22873 22959 ## + sex 1 22885 22965 ## &lt;none&gt; 22887 22965 ## - hours_week_bin 2 23327 23401 ## - occupation 12 23593 23647 ## - age_bin 3 23587 23659 ## - capital_gain_indicator 1 23878 23954 ## - education_level 14 24069 24119 ## - marital_status 6 27130 27196 ## ## Step: AIC=22678.02 ## over_50k ~ marital_status + education_level + capital_gain_indicator + ## occupation + age_bin + hours_week_bin + capital_loss_indicator ## ## Df Deviance AIC ## + workclass 6 22468 22560 ## + country_bin 1 22575 22657 ## + race 4 22585 22673 ## + sex 1 22596 22678 ## &lt;none&gt; 22598 22678 ## - capital_loss_indicator 1 22887 22965 ## - hours_week_bin 2 23026 23102 ## - occupation 12 23285 23341 ## - age_bin 3 23277 23351 ## - capital_gain_indicator 1 23684 23762 ## - education_level 14 23723 23775 ## - marital_status 6 26776 26844 ## ## Step: AIC=22559.66 ## over_50k ~ marital_status + education_level + capital_gain_indicator + ## occupation + age_bin + hours_week_bin + capital_loss_indicator + ## workclass ## ## Df Deviance AIC ## + country_bin 1 22443 22537 ## + race 4 22453 22553 ## &lt;none&gt; 22468 22560 ## + sex 1 22466 22560 ## - workclass 6 22598 22678 ## - capital_loss_indicator 1 22756 22846 ## - hours_week_bin 2 22864 22952 ## - occupation 12 23061 23129 ## - age_bin 3 23151 23237 ## - capital_gain_indicator 1 23542 23632 ## - education_level 14 23611 23675 ## - marital_status 6 26627 26707 ## ## Step: AIC=22537.28 ## over_50k ~ marital_status + education_level + capital_gain_indicator + ## occupation + age_bin + hours_week_bin + capital_loss_indicator + ## workclass + country_bin ## ## Df Deviance AIC ## &lt;none&gt; 22443 22537 ## + race 4 22436 22538 ## + sex 1 22442 22538 ## - country_bin 1 22468 22560 ## - workclass 6 22575 22657 ## - capital_loss_indicator 1 22729 22821 ## - hours_week_bin 2 22833 22923 ## - occupation 12 23023 23093 ## - age_bin 3 23123 23211 ## - capital_gain_indicator 1 23513 23605 ## - education_level 14 23588 23654 ## - marital_status 6 26612 26694 # Forward selection to select two-variable interactions # First: double check interactions for quasi complete separation table(train$over_50k, train$hours_week_bin, train$occupation) ## , , = ? ## ## ## 0 1 2 ## 0 931 669 190 ## 1 73 66 51 ## ## , , = Adm-clerical ## ## ## 0 1 2 ## 0 1045 1907 412 ## 1 82 318 147 ## ## , , = Craft-repair ## ## ## 0 1 2 ## 0 419 1995 869 ## 1 39 516 412 ## ## , , = Exec-managerial ## ## ## 0 1 2 ## 0 353 1051 836 ## 1 121 706 1223 ## ## , , = Farming-fishing ## ## ## 0 1 2 ## 0 200 285 423 ## 1 8 31 81 ## ## , , = Handlers-cleaners ## ## ## 0 1 2 ## 0 460 807 226 ## 1 8 67 27 ## ## , , = Machine-op-inspct ## ## ## 0 1 2 ## 0 227 1303 368 ## 1 8 163 90 ## ## , , = Other-service ## ## ## 0 1 2 ## 0 1627 1277 383 ## 1 32 62 46 ## ## , , = Prof-specialty ## ## ## 0 1 2 ## 0 646 1010 704 ## 1 248 755 946 ## ## , , = Protective-serv ## ## ## 0 1 2 ## 0 95 270 113 ## 1 8 126 96 ## ## , , = Sales ## ## ## 0 1 2 ## 0 1048 943 869 ## 1 88 332 593 ## ## , , = Tech-support ## ## ## 0 1 2 ## 0 177 403 132 ## 1 41 169 91 ## ## , , = Transport-moving ## ## ## 0 1 2 ## 0 193 625 473 ## 1 26 126 203 table(train$over_50k, train$race, train$country_bin) # Quasi complete separation!! ## , , = 0 ## ## ## Amer-Indian-Eskimo Asian-Pac-Islander Black Other White ## 0 12 553 264 128 1884 ## 1 1 185 34 19 435 ## ## , , = 1 ## ## ## Amer-Indian-Eskimo Asian-Pac-Islander Black Other White ## 0 250 213 2646 115 19899 ## 1 39 80 375 17 7040 table(train$over_50k, train$sex, train$capital_gain_indicator) ## , , = 0 ## ## ## Female Male ## 0 9707 15200 ## 1 950 5499 ## ## , , = 1 ## ## ## Female Male ## 0 364 693 ## 1 307 1469 # Build main model with stepwise selected variables main.model &lt;- glm(over_50k ~ marital_status + education_level + capital_gain_indicator + occupation + hours_week_bin + age_bin + capital_loss_indicator + workclass + country_bin + race, data = train, family = binomial(link = &quot;logit&quot;)) # Build model with interactions of interest int.model &lt;- glm(over_50k ~ marital_status + education_level + capital_gain_indicator + occupation + hours_week_bin + age_bin + capital_loss_indicator + workclass + country_bin + race + hours_week_bin*occupation + sex*capital_gain_indicator, data = train, family = binomial(link = &quot;logit&quot;)) # Forward selection for.model &lt;- step(main.model, scope = list(lower=formula(main.model), upper=formula(int.model)), direction = &quot;forward&quot;) ## Start: AIC=22537.6 ## over_50k ~ marital_status + education_level + capital_gain_indicator + ## occupation + hours_week_bin + age_bin + capital_loss_indicator + ## workclass + country_bin + race ## ## Df Deviance AIC ## &lt;none&gt; 22436 22538 ## + sex 1 22434 22538 ## + occupation:hours_week_bin 24 22392 22542 # Final model: # over_50k ~ marital_status + education_level + capital_gain_indicator + # occupation + hours_week_bin + age_bin + capital_loss_indicator + # workclass + country_bin + race + occupation*hours_week_bin 3.3 Create Logistic Regression # GLM with binomial logit link logit.model &lt;- glm(over_50k ~ marital_status + education_level + capital_gain_indicator + occupation + hours_week_bin + age_bin + capital_loss_indicator + workclass + country_bin + race + occupation*hours_week_bin, data = train, family = binomial(link = &quot;logit&quot;)) summary(logit.model) ## ## Call: ## glm(formula = over_50k ~ marital_status + education_level + capital_gain_indicator + ## occupation + hours_week_bin + age_bin + capital_loss_indicator + ## workclass + country_bin + race + occupation * hours_week_bin, ## family = binomial(link = &quot;logit&quot;), data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.7530 -0.5064 -0.1972 -0.0498 3.4410 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.951257 0.955506 -7.275 3.47e-13 *** ## marital_statusMarried-AF-spouse 2.793930 0.513582 5.440 5.33e-08 *** ## marital_statusMarried-civ-spouse 2.295307 0.059831 38.363 &lt; 2e-16 *** ## marital_statusMarried-spouse-absent 0.177184 0.208600 0.849 0.39566 ## marital_statusNever-married -0.170596 0.077262 -2.208 0.02724 * ## marital_statusSeparated -0.151642 0.155739 -0.974 0.33021 ## marital_statusWidowed 0.273052 0.135154 2.020 0.04335 * ## education_level11th 0.180145 0.201741 0.893 0.37188 ## education_level12th 0.494708 0.253626 1.951 0.05111 . ## education_level5th-6th -0.109677 0.292222 -0.375 0.70742 ## education_level7th-8th -0.192605 0.219436 -0.878 0.38009 ## education_level9th -0.200234 0.251777 -0.795 0.42645 ## education_levelAssoc-acdm 1.428494 0.171087 8.350 &lt; 2e-16 *** ## education_levelAssoc-voc 1.368513 0.164603 8.314 &lt; 2e-16 *** ## education_levelBachelors 1.957648 0.153465 12.756 &lt; 2e-16 *** ## education_levelDoctorate 2.971492 0.204201 14.552 &lt; 2e-16 *** ## education_levelEarly-Ed -0.646743 0.435116 -1.486 0.13718 ## education_levelHS-grad 0.823883 0.149487 5.511 3.56e-08 *** ## education_levelMasters 2.349794 0.162945 14.421 &lt; 2e-16 *** ## education_levelProf-school 3.006142 0.195727 15.359 &lt; 2e-16 *** ## education_levelSome-college 1.217636 0.151634 8.030 9.74e-16 *** ## capital_gain_indicator 1.733140 0.054647 31.715 &lt; 2e-16 *** ## occupationAdm-clerical 1.018234 0.918734 1.108 0.26773 ## occupationCraft-repair 0.668285 0.930504 0.718 0.47264 ## occupationExec-managerial 1.598319 0.920709 1.736 0.08257 . ## occupationFarming-fishing -0.066459 0.991402 -0.067 0.94655 ## occupationHandlers-cleaners -0.246903 0.993893 -0.248 0.80381 ## occupationMachine-op-inspct 0.088021 0.988750 0.089 0.92906 ## occupationOther-service 0.100080 0.931026 0.107 0.91440 ## occupationProf-specialty 1.437418 0.916742 1.568 0.11689 ## occupationProtective-serv 0.223285 1.005657 0.222 0.82429 ## occupationSales 1.085443 0.921320 1.178 0.23874 ## occupationTech-support 1.657876 0.936830 1.770 0.07678 . ## occupationTransport-moving 1.110413 0.941569 1.179 0.23827 ## hours_week_bin1 0.536663 0.208611 2.573 0.01009 * ## hours_week_bin2 1.170019 0.247269 4.732 2.23e-06 *** ## age_bin1 1.073278 0.059826 17.940 &lt; 2e-16 *** ## age_bin2 1.516964 0.061723 24.577 &lt; 2e-16 *** ## age_bin3 1.285635 0.071977 17.862 &lt; 2e-16 *** ## capital_loss_indicator 1.153914 0.068817 16.768 &lt; 2e-16 *** ## workclassLocal-gov -0.672888 0.107391 -6.266 3.71e-10 *** ## workclassPrivate -0.451551 0.090157 -5.008 5.49e-07 *** ## workclassSelf-emp-inc -0.184345 0.117607 -1.567 0.11700 ## workclassSelf-emp-not-inc -0.916236 0.106503 -8.603 &lt; 2e-16 *** ## workclassState-gov -0.828250 0.119387 -6.938 3.99e-12 *** ## workclassUnknown -0.089842 0.904776 -0.099 0.92090 ## country_bin 0.283528 0.068012 4.169 3.06e-05 *** ## raceAsian-Pac-Islander 0.240651 0.233835 1.029 0.30341 ## raceBlack 0.184810 0.216739 0.853 0.39383 ## raceOther 0.318731 0.307788 1.036 0.30041 ## raceWhite 0.330704 0.207070 1.597 0.11025 ## occupationAdm-clerical:hours_week_bin1 -0.107592 0.258051 -0.417 0.67672 ## occupationCraft-repair:hours_week_bin1 0.260847 0.286670 0.910 0.36286 ## occupationExec-managerial:hours_week_bin1 0.001299 0.254092 0.005 0.99592 ## occupationFarming-fishing:hours_week_bin1 0.336487 0.493488 0.682 0.49533 ## occupationHandlers-cleaners:hours_week_bin1 0.611267 0.469276 1.303 0.19272 ## occupationMachine-op-inspct:hours_week_bin1 0.421233 0.446426 0.944 0.34539 ## occupationOther-service:hours_week_bin1 -0.104592 0.315649 -0.331 0.74038 ## occupationProf-specialty:hours_week_bin1 0.139985 0.238569 0.587 0.55736 ## occupationProtective-serv:hours_week_bin1 1.445197 0.490787 2.945 0.00323 ** ## occupationSales:hours_week_bin1 0.113005 0.260881 0.433 0.66489 ## occupationTech-support:hours_week_bin1 -0.260118 0.321875 -0.808 0.41901 ## occupationTransport-moving:hours_week_bin1 -0.333114 0.332977 -1.000 0.31711 ## occupationAdm-clerical:hours_week_bin2 -0.200207 0.305719 -0.655 0.51255 ## occupationCraft-repair:hours_week_bin2 0.153410 0.318342 0.482 0.62988 ## occupationExec-managerial:hours_week_bin2 -0.037527 0.285373 -0.132 0.89538 ## occupationFarming-fishing:hours_week_bin2 -0.077143 0.481868 -0.160 0.87281 ## occupationHandlers-cleaners:hours_week_bin2 0.129536 0.522541 0.248 0.80421 ## occupationMachine-op-inspct:hours_week_bin2 0.265679 0.475708 0.558 0.57651 ## occupationOther-service:hours_week_bin2 -0.035817 0.361081 -0.099 0.92098 ## occupationProf-specialty:hours_week_bin2 -0.358254 0.273382 -1.310 0.19004 ## occupationProtective-serv:hours_week_bin2 1.212776 0.520659 2.329 0.01984 * ## occupationSales:hours_week_bin2 -0.153217 0.288892 -0.530 0.59586 ## occupationTech-support:hours_week_bin2 -0.189976 0.370356 -0.513 0.60798 ## occupationTransport-moving:hours_week_bin2 -0.343914 0.354885 -0.969 0.33250 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 37727 on 34188 degrees of freedom ## Residual deviance: 22392 on 34114 degrees of freedom ## AIC: 22542 ## ## Number of Fisher Scoring iterations: 7 3.4 Evaluate logistic regression View Coefficient of Discrimination (R2) # Get coefficient of discrimination (R2) train$p_hat = predict(logit.model, type = &#39;response&#39;) p1 = train$p_hat[train$over_50k == 1] p0 = train$p_hat[train$over_50k == 0] coef_discrim = mean(p1) - mean(p0) print(coef_discrim) ## [1] 0.4294047 # Coeff of discrimination = 0.422 # Get proportions of non-buy and buy prop0 = 26037/34189 prop1 = 8152/34189 # Plot probabilities as density plot ggplot(train, aes(p_hat, fill = over_50k)) + geom_density(alpha = 0.7) + labs(x = &quot;Predicted Probability&quot;, y = &quot;Density&quot;, fill = &quot;Outcome&quot;, title = paste(&quot;Coefficient of Discrimination = &quot;, round(coef_discrim, 3), sep = &quot;&quot;))+ scale_fill_manual( values = c(&quot;royalblue&quot;,&quot;skyblue&quot;), labels=c(&quot;Not Over 50k&quot;, &quot;Over 50k&quot;))+ theme_classic() Determine optimal cut-off # Iterate through cut-off values to determine optimal cut-off train$p_hat &lt;- predict(logit.model, type = &quot;response&quot;) youden &lt;- NULL cutoff &lt;- NULL for(i in 1:49){ cutoff = c(cutoff, i/50) youden &lt;- c(youden, youdensIndex(train$over_50k, train$p_hat, threshold = i/50)) } # Print table with lowest Youdens at the top of the list ctable &lt;- data.frame(cutoff, youden) print(ctable[order(-youden),]) ## cutoff youden ## 11 0.22 0.642762787 ## 12 0.24 0.641031913 ## 10 0.20 0.638109391 ## 9 0.18 0.635488947 ## 13 0.26 0.633766679 ## 14 0.28 0.632704643 ## 8 0.16 0.627697738 ## 15 0.30 0.627053587 ## 16 0.32 0.622357644 ## 7 0.14 0.620571631 ## 17 0.34 0.609694293 ## 6 0.12 0.607149778 ## 18 0.36 0.599626385 ## 19 0.38 0.592510902 ## 5 0.10 0.585179924 ## 20 0.40 0.579837493 ## 21 0.42 0.571981893 ## 22 0.44 0.560341488 ## 4 0.08 0.559639398 ## 23 0.46 0.544290266 ## 24 0.48 0.534006792 ## 3 0.06 0.523481702 ## 25 0.50 0.518637342 ## 26 0.52 0.507304193 ## 27 0.54 0.492220156 ## 28 0.56 0.475928648 ## 29 0.58 0.466394203 ## 2 0.04 0.453325151 ## 30 0.60 0.451097184 ## 31 0.62 0.438650842 ## 32 0.64 0.422295814 ## 33 0.66 0.395848163 ## 34 0.68 0.376495873 ## 35 0.70 0.356969116 ## 1 0.02 0.342111177 ## 36 0.72 0.341594206 ## 37 0.74 0.320142568 ## 38 0.76 0.292976644 ## 39 0.78 0.272720405 ## 40 0.80 0.244190652 ## 41 0.82 0.219543141 ## 42 0.84 0.201402058 ## 43 0.86 0.170474611 ## 44 0.88 0.145490239 ## 45 0.90 0.122436785 ## 46 0.92 0.094591103 ## 47 0.94 0.070295401 ## 48 0.96 0.038443597 ## 49 0.98 0.008510638 # Confusion matrix for train using Youden&#39;s Index optimal cut off train$classification = ifelse(train$p_hat &gt;= 0.2, 1, 0) confusionMatrix(train$over_50k, factor(train$classification)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 19753 6211 ## 1 1009 7216 ## ## Accuracy : 0.7888 ## 95% CI : (0.7845, 0.7931) ## No Information Rate : 0.6073 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.5247 ## ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Sensitivity : 0.9514 ## Specificity : 0.5374 ## Pos Pred Value : 0.7608 ## Neg Pred Value : 0.8773 ## Prevalence : 0.6073 ## Detection Rate : 0.5778 ## Detection Prevalence : 0.7594 ## Balanced Accuracy : 0.7444 ## ## &#39;Positive&#39; Class : 0 ## AUROC # Evaluate model using AUROC train$p_hat &lt;- predict(logit.model, type = &quot;response&quot;) plotROC(train$over_50k, train$p_hat) # 0.902 AUROC 3.5 Test training cut off on validation # Make predictions on validation set validation$p_hat = predict(logit.model, newdata = validation, type = &#39;response&#39;) # Confusion Matrix using Youdens cutoff validation$classification = ifelse(validation$p_hat &gt;= 0.2, 1, 0) confusionMatrix(validation$over_50k, factor(validation$classification)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 2838 863 ## 1 161 1022 ## ## Accuracy : 0.7903 ## 95% CI : (0.7786, 0.8017) ## No Information Rate : 0.614 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.5248 ## ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Sensitivity : 0.9463 ## Specificity : 0.5422 ## Pos Pred Value : 0.7668 ## Neg Pred Value : 0.8639 ## Prevalence : 0.6140 ## Detection Rate : 0.5811 ## Detection Prevalence : 0.7578 ## Balanced Accuracy : 0.7442 ## ## &#39;Positive&#39; Class : 0 ## # Get concordance Concordance(validation$over_50k, validation$p_hat) ## $Concordance ## [1] 0.9016553 ## ## $Discordance ## [1] 0.09834472 ## ## $Tied ## [1] -4.163336e-17 ## ## $Pairs ## [1] 4378283 # Evaluate model using AUROC plotROC(validation$over_50k, validation$p_hat) # 0.899 AUROC "],["create-xgboost.html", "Chapter 4 Create XGBoost 4.1 Tuning an XGBoost nrounds parameter - 11 was lowest 4.2 Tuning through caret 4.3 Variable importance 4.4 ROC Curve and AUC", " Chapter 4 Create XGBoost 4.1 Tuning an XGBoost nrounds parameter - 11 was lowest 4.2 Tuning through caret eta = .9 and subsample = .25, max tree depth = 10 4.3 Variable importance xgb &lt;- xgboost(data = train_x, label = train_y, subsample = .25, nrounds = 11, eta = 0.9, max_depth = 10) ## [1] train-rmse:0.340196 ## [2] train-rmse:0.335860 ## [3] train-rmse:0.336339 ## [4] train-rmse:0.337874 ## [5] train-rmse:0.339194 ## [6] train-rmse:0.338953 ## [7] train-rmse:0.340051 ## [8] train-rmse:0.339826 ## [9] train-rmse:0.340647 ## [10] train-rmse:0.341934 ## [11] train-rmse:0.342539 xgb.importance(feature_names = colnames(train_x), model = xgb) ## Feature Gain Cover Frequency ## 1: marital_statusMarried-civ-spouse 0.210253784 0.0562343621 0.047082228 ## 2: capital_gain_indicator 0.063878144 0.0409333845 0.052718833 ## 3: education_levelBachelors 0.054371522 0.0433295325 0.030835544 ## 4: occupationProf-specialty 0.043213945 0.0447810556 0.021551724 ## 5: occupationExec-managerial 0.041014177 0.0326252249 0.023541114 ## 6: hours_week_bin2 0.034572953 0.0278902117 0.041445623 ## 7: age_bin2 0.034069255 0.0335503953 0.051392573 ## 8: capital_loss_indicator 0.031711947 0.0374142785 0.039124668 ## 9: education_levelMasters 0.029735995 0.0484730906 0.025198939 ## 10: age_bin1 0.024941425 0.0435856836 0.052718833 ## 11: sexMale 0.022629657 0.0100796014 0.037798408 ## 12: hours_week_bin1 0.022520770 0.0229044513 0.058023873 ## 13: workclassPrivate 0.021677904 0.0193821029 0.042440318 ## 14: education_levelHS-grad 0.021303070 0.0082519576 0.026525199 ## 15: workclassSelf-emp-not-inc 0.020814088 0.0101422882 0.020225464 ## 16: age_bin3 0.018453215 0.0343804547 0.031498674 ## 17: country_bin 0.017937683 0.0209276562 0.025530504 ## 18: marital_statusNever-married 0.016527463 0.0149702508 0.029177719 ## 19: education_levelDoctorate 0.016246897 0.0407939605 0.013594164 ## 20: occupationCraft-repair 0.015893951 0.0277486260 0.018567639 ## 21: education_levelSome-college 0.015794056 0.0096407940 0.022214854 ## 22: education_levelAssoc-voc 0.012792826 0.0120607197 0.019562334 ## 23: workclassLocal-gov 0.012671833 0.0306624803 0.021551724 ## 24: raceAsian-Pac-Islander 0.012659613 0.0298842997 0.017904509 ## 25: workclassState-gov 0.012447971 0.0189162753 0.019893899 ## 26: workclassSelf-emp-inc 0.012328598 0.0252714175 0.017904509 ## 27: education_levelProf-school 0.011200841 0.0358449475 0.009283820 ## 28: occupationSales 0.010900126 0.0155776641 0.013262599 ## 29: occupationAdm-clerical 0.010771598 0.0080714629 0.013925729 ## 30: raceBlack 0.010723369 0.0045545186 0.016578249 ## 31: education_levelAssoc-acdm 0.010287803 0.0037709339 0.010941645 ## 32: occupationOther-service 0.008729316 0.0050797905 0.007294430 ## 33: raceWhite 0.008616448 0.0048722757 0.012267905 ## 34: marital_statusSeparated 0.008579805 0.0044669733 0.010278515 ## 35: occupationProtective-serv 0.008496345 0.0250131048 0.007957560 ## 36: occupationTech-support 0.008351747 0.0202834955 0.008952255 ## 37: occupationTransport-moving 0.008059925 0.0376682681 0.006962865 ## 38: occupationFarming-fishing 0.006672579 0.0069322929 0.004973475 ## 39: occupationHandlers-cleaners 0.006505596 0.0151507455 0.006631300 ## 40: marital_statusWidowed 0.005578882 0.0022869865 0.009615385 ## 41: education_level12th 0.005126731 0.0089966333 0.005968170 ## 42: workclassUnknown 0.004779542 0.0040940950 0.005636605 ## 43: occupationMachine-op-inspct 0.004500412 0.0042302766 0.003315650 ## 44: education_level7th-8th 0.004060411 0.0064362027 0.006631300 ## 45: raceOther 0.003660906 0.0019357244 0.004310345 ## 46: education_level9th 0.003416557 0.0063864856 0.007294430 ## 47: marital_statusMarried-AF-spouse 0.002992914 0.0279993731 0.003978780 ## 48: education_level11th 0.002728660 0.0022167341 0.005636605 ## 49: marital_statusMarried-spouse-absent 0.002582004 0.0023604814 0.005305040 ## 50: education_levelEarly-Ed 0.001116936 0.0004323226 0.002320955 ## 51: education_level5th-6th 0.001097802 0.0005036558 0.002652520 ## Feature Gain Cover Frequency xgb.ggplot.importance(xgb.importance(feature_names = colnames(train_x), model = xgb)) 4.4 ROC Curve and AUC # get predictions # Prepare data for predict function validation_x &lt;- model.matrix(over_50k ~ ., data = validation)[, 3:53] validation_y &lt;- as.numeric(as.character(validation$over_50k)) p_hat_xgb &lt;- predict(xgb, newdata = validation_x, type = &quot;response&quot;) # create ROC object rocobj &lt;- roc(validation$over_50k, p_hat_xgb) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases rocobj$auc # to get AUC - 0.816 ## Area under the curve: 0.8242 # create ROC plot with minimal theme ggroc(rocobj, colour = &#39;steelblue&#39;, size = 2) + ggtitle(paste0(&#39;XGBoost Model ROC Curve &#39;, &#39;(AUC = &#39;, round(rocobj$auc, 4), &#39;)&#39;)) + theme_minimal()+ labs(x=&quot;1-Specificity (FPR)&quot;,y=&quot;Sensitivity (TPR)&quot;)+geom_abline(intercept = 1.00, size = 0.5, linetype=&quot;dotted&quot;, color = &quot;red&quot;)+ coord_cartesian(xlim=c(1,0)) "],["conclusion-write-up.html", "Chapter 5 Conclusion / Write Up 5.1 Figure 1 5.2 Final AUROC Chart", " Chapter 5 Conclusion / Write Up Overview The goal of this project is to develop a model that predicts whether individuals, based on census variables, make over $50,000/year. I compared the predictive power of a logistic regression to an XGBoost model using the area under the ROC curve (AUROC). This statistic calculates each models ability to distinguish between people who make above $50,000/year from those who do not. The logistic regression model resulted in the higher AUROC, and was able to better predict who is likely to make above $50,000/year from census data. The final AUROC on the test data was 0.894. Methodology The first two steps were exploring each variable individually and binning continuous variables into categories. Each variable was then tested for quasi-complete separation. This is a problem when a subgroup of the predictor variable perfectly predicts the target variable outcome. Each subgroup with less than 5 observations was collapsed into a new, larger group. The data was split into a 70/20/10 training/validation/test split and each variable was tested for multi-collinearity to ensure no two variables are providing the same information in the model. Stepwise selection was used to identify which variables were important in predicting the target. The 13 important variables were then used in a forward selection process to identify any predictive interactions. For example, the selection process identified changes in the target variables relationship with hours worked each week when looking at different occupations. A logistic regression was built using these variables. The XGBoost was tuned and built using all census variables, and the most important variables were calculated using a gain statistic. Results A simple relationship between marital status and people who make over 50k can be pictured in Figure 1. This figure shows the large majority of people who make over 50k are married to civil spouses. In our census data, 85% of people who make over 50k are married to a civil spouse compared to only 33% of those who do not make over 50k. Furthermore, the XGBoost model found marital status to be the most important variable in predicting the target. The logistic regression was able to better predict people who make over $50,000/year using census variables. I recommend using a logistic regression moving forward to best predict the individuals that make over 50K using census data. Given more time for tuning, you might be able to challenge the predictive power using a future XGBoost model. 5.1 Figure 1 # Add color blind palette cbPalette = c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;) # Plot data ggplot(data = train) + geom_bar(mapping = aes(x = over_50k, fill = marital_status, color =)) + scale_fill_manual(values=cbPalette) + #scale_fill_discrete(labels = c(&quot;Divorced&quot;, &quot;Married- Armed Forces&quot;, &quot;Married- Civil Spouse&quot;, &quot;Married- Spouse Absent&quot;, &quot;Never Married&quot;, &quot;Separated&quot;, &quot;Widowed&quot;)) + labs(title=&quot;Marital Status over Earning Category&quot;, x =&quot;Earning Over 50k (0: No, 1: Yes)&quot;, y = &quot;Frequency&quot;, fill = &quot;Marital Status&quot;) + theme_minimal() 5.2 Final AUROC Chart # Make predictions on validation set test$p_hat = predict(logit.model, newdata = test, type = &#39;response&#39;) # Evaluate model using AUROC plotROC(test$over_50k, test$p_hat) # 0.894 AUROC This repo was initially generated from a bookdown template available here: https://github.com/jtr13/bookdown-template. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
